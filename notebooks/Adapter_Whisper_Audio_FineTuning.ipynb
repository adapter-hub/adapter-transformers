{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Whisper using Adapters\n",
    "\n",
    "In this tutorial, we will be demonstrating how to fine-tune a [Whisper](https://arxiv.org/abs/2212.04356) Model using the adapters framework. We will be adding [LoRA adapters](https://docs.adapterhub.ml/methods#lora) into the Whisper Model while freezing the model weights. Then we will incorporate a sequence to sequence head on top of the model so that we can do speech recognition. This tutorial is widely inspired by the [Whisper PEFT-Lora blog post](https://github.com/huggingface/peft/blob/main/examples/int8_training/peft_bnb_whisper_large_v2_training.ipynb), be sure to check it out for more details on ``Whisper`` fine-tuning. \n",
    "\n",
    "For more information on the Whisper Model, please visit the huggingface model card https://huggingface.co/openai/whisper-large-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Before we can get started with the model, we need to ensure the proper packages are installed. Ensure you have `accelerate`, `bitsandbytes` and `datasets` installed along with the `adapters` library and various speech recognition libraries as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\jackson\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.6.2)\n",
      "Requirement already satisfied: jiwer in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (3.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\jackson\\appdata\\roaming\\python\\python312\\site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from jiwer) (3.9.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n",
      "Requirement already satisfied: gradio in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (5.3.0)\n",
      "Requirement already satisfied: fastapi in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.111.0)\n",
      "Requirement already satisfied: ffmpy in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==1.0.2 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (1.0.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.23.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (3.9.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (3.10.6)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (10.2.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (2.8.2)\n",
      "Requirement already satisfied: pydub in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio) (0.30.1)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio-client==1.0.2->gradio) (2024.5.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from gradio-client==1.0.2->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.22.0)\n",
      "Requirement already satisfied: toolz in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from httpx>=0.24.1->gradio) (4.4.0)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: idna in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jackson\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from fastapi->gradio) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from fastapi->gradio) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from fastapi->gradio) (2.2.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\envs\\adapters_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "!pip install evaluate>=0.30\n",
    "!pip install jiwer\n",
    "!pip install gradio\n",
    "!pip install -qq -U adapters accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Configuration\n",
    "\n",
    "In this tutorial, we will be using the mozilla-foundation/common_voice_11_0 dataset. In this cell, we set the proper cuda device to leverage the GPU and also set some of the dataset configurations. You can always change the below config to select what datasets you prefer the adapters model to train on. More infomation on the common voice dataset can be found [here](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUDA device index\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name_or_path = \"openai/whisper-tiny\"\n",
    "language = \"cantonese\"\n",
    "language_abbr = \"zh-HK\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "We load the dataset and split it into its respective train and test sets. We then remove some of the columns as they are not needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\datasets\\load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\datasets\\load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 8423\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 5591\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-HK\", split=\"train\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"zh-HK\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\Jackson\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\38272d4d8a5becb490327bdb81aef3d7a11ae9499ba16e02965a27567411ad93\\\\zh-HK_train_0/common_voice_zh-HK_22942304.mp3', 'array': array([ 0.00000000e+00,  4.30017540e-13,  6.87821111e-13, ...,\n",
      "       -1.39293297e-06, -6.22257721e-06, -1.14162267e-05]), 'sampling_rate': 48000}, 'sentence': '才能勇往直前'}\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "\n",
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Feature Extractor, Tokenizer and Processor\n",
    "\n",
    "These modules are required and work specifically for the `Whisper` Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "This section is dedicated to pre-processing the dataset we loaded into something that the model can use to train and learn from. \n",
    "\n",
    "The `Whisper` model expects the sample rate to be 16000 hz, while the audio in the dataset is set at 48000 hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample down to 16000\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\Jackson\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\38272d4d8a5becb490327bdb81aef3d7a11ae9499ba16e02965a27567411ad93\\\\zh-HK_train_0/common_voice_zh-HK_22942304.mp3', 'array': array([ 5.45696821e-12,  2.72848411e-12,  3.63797881e-12, ...,\n",
      "        1.48210138e-05,  9.73203896e-07, -4.09249424e-06]), 'sampling_rate': 16000}, 'sentence': '才能勇往直前'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare a function `prepare_dataset` that will take in a batch of samples and process inputs and labels.\n",
    "\n",
    "In `prepare_dataset` we:\n",
    "1) Grab the audio data from each sample in the batch\n",
    "2) Create a new column named `input_features` that contain the extracted features when calling the `WhisperFeatureExtractor` onto the audio data\n",
    "3) Create a new column called `labels` which contain the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the built in `map' function to build our dataset using the pre-processing function before passing it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the DataCollator\n",
    "\n",
    "We now initialize a DataCollator that will be responsible for setting up a data preprocessing pipeline designed for sequence to sequence data.\n",
    "\n",
    "In the DataCollator we ensure that both the input features and our tokenized input_ids in our labels are of the same length. We do this by padding both of them to ensure they are equal, and then replace the padding values with -100 to ensure their loss values are ignored during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize our DataCollator so we can apply it to our dataset before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "We'll use the word error rate (WER) metric, a metric used primarily for evaluating performance on audio speech recognition models. For more information, please go to the WER [docs](https://huggingface.co/metrics/wer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Github\\adapters\n",
      "d:\\Documents\\Github\\adapters\\src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d:\\\\Documents\\\\Github\\\\adapters\\\\src'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some temp code to get into the right directory for imports\n",
    "import os\n",
    "%cd ..\n",
    "%cd src\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the `Whisper` adapters model\n",
    "\n",
    "Here we will initialize the adapters `Whisper` model.\n",
    "\n",
    "`adapters` acts like a wrapper to the `transformers` library so we can directly use the `WhisperConfig` for model specifications. From the `adapters` module we then import the `WhisperAdapterModel` and intialize it using the config we previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of WhisperAdapterModel were not initialized from the model checkpoint at openai/whisper-tiny and are newly initialized: ['heads.default.0.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperConfig\n",
    "from adapters.models.whisper.adapter_model import WhisperAdapterModel #will need replacing\n",
    "config = WhisperConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")\n",
    "model = WhisperAdapterModel.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add an untrained adapter named `whisper_adapter` that we will fine-tune instead of the ``Whisper`` model parameters. You can always choose another adapter from the [Adapter Hub](https://adapterhub.ml/) to suit your needs. Afterwards we include a sequence to sequence model head so we can generate tokens from the ``Whisper`` model.\n",
    "\n",
    "Once these two components are added, we leverage the `train_adapter` function to make sure `adapters` knows that these are the weights that need to be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "whisper_adapter          lora              3,735,552       9.893       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                                37,760,640     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import adapters\n",
    "#initialize the Lora Config to use as an adapter\n",
    "from adapters import LoRAConfig\n",
    "\n",
    "adapters.init(model)\n",
    "\n",
    "config = LoRAConfig(\n",
    "    selfattn_lora=True, intermediate_lora=True, output_lora=True,\n",
    "    attn_matrices=[\"q\", \"k\", \"v\"],\n",
    "    alpha=16, r=64, dropout=0.1\n",
    ")\n",
    "model.add_adapter(\"whisper_adapter\", config=config)\n",
    "model.add_seq2seq_lm_head(\"whisper_adapter\")\n",
    "model.train_adapter(\"whisper_adapter\")\n",
    "\n",
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"temp\",  # change to a directory name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1, #edit this based on the number of epochs you would like to train\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=25,\n",
    "    remove_unused_columns=False, \n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the `adapters` sequence to sequence adapter trainer. This works very similarly to the `Trainer` module inside `huggingface`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import Seq2SeqAdapterTrainer\n",
    "\n",
    "trainer = Seq2SeqAdapterTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0451f354e126439c80ce3306bde928fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Github\\adapters\\src\\adapters\\models\\whisper\\modeling_whisper.py:380: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9546, 'grad_norm': 2.2400691509246826, 'learning_rate': 0.0005, 'epoch': 0.02}\n",
      "{'loss': 0.627, 'grad_norm': 2.3768436908721924, 'learning_rate': 0.001, 'epoch': 0.05}\n",
      "{'loss': 0.7108, 'grad_norm': 2.8131179809570312, 'learning_rate': 0.0009750747756729811, 'epoch': 0.07}\n",
      "{'loss': 0.7192, 'grad_norm': 2.126743793487549, 'learning_rate': 0.0009501495513459621, 'epoch': 0.09}\n",
      "{'loss': 0.7509, 'grad_norm': 3.2347044944763184, 'learning_rate': 0.000926221335992024, 'epoch': 0.12}\n",
      "{'loss': 0.7545, 'grad_norm': 3.0252304077148438, 'learning_rate': 0.000901296111665005, 'epoch': 0.14}\n",
      "{'loss': 0.7089, 'grad_norm': 2.927863121032715, 'learning_rate': 0.0008763708873379861, 'epoch': 0.17}\n",
      "{'loss': 0.7539, 'grad_norm': 2.5152058601379395, 'learning_rate': 0.0008514456630109671, 'epoch': 0.19}\n",
      "{'loss': 0.6881, 'grad_norm': 2.572906732559204, 'learning_rate': 0.000827517447657029, 'epoch': 0.21}\n",
      "{'loss': 0.7325, 'grad_norm': 3.0144991874694824, 'learning_rate': 0.0008025922233300101, 'epoch': 0.24}\n",
      "{'loss': 0.7812, 'grad_norm': 4.132767677307129, 'learning_rate': 0.0007776669990029911, 'epoch': 0.26}\n",
      "{'loss': 0.8105, 'grad_norm': 4.141232013702393, 'learning_rate': 0.000752741774675972, 'epoch': 0.28}\n",
      "{'loss': 0.6953, 'grad_norm': 3.426758289337158, 'learning_rate': 0.0007278165503489531, 'epoch': 0.31}\n",
      "{'loss': 0.708, 'grad_norm': 2.996276617050171, 'learning_rate': 0.0007028913260219341, 'epoch': 0.33}\n",
      "{'loss': 0.6472, 'grad_norm': 2.3680903911590576, 'learning_rate': 0.0006779661016949152, 'epoch': 0.36}\n",
      "{'loss': 0.6409, 'grad_norm': 2.5756239891052246, 'learning_rate': 0.0006530408773678963, 'epoch': 0.38}\n",
      "{'loss': 0.6768, 'grad_norm': 3.384721279144287, 'learning_rate': 0.0006281156530408774, 'epoch': 0.4}\n",
      "{'loss': 0.6156, 'grad_norm': 4.924454212188721, 'learning_rate': 0.0006031904287138584, 'epoch': 0.43}\n",
      "{'loss': 0.6498, 'grad_norm': 2.4422411918640137, 'learning_rate': 0.0005782652043868395, 'epoch': 0.45}\n",
      "{'loss': 0.6014, 'grad_norm': 2.5655100345611572, 'learning_rate': 0.0005533399800598205, 'epoch': 0.47}\n",
      "{'loss': 0.5491, 'grad_norm': 2.172391653060913, 'learning_rate': 0.0005284147557328016, 'epoch': 0.5}\n",
      "{'loss': 0.5967, 'grad_norm': 2.414074659347534, 'learning_rate': 0.0005034895314057826, 'epoch': 0.52}\n",
      "{'loss': 0.5382, 'grad_norm': 1.9657586812973022, 'learning_rate': 0.0004785643070787637, 'epoch': 0.55}\n",
      "{'loss': 0.5723, 'grad_norm': 4.889124393463135, 'learning_rate': 0.00045363908275174473, 'epoch': 0.57}\n",
      "{'loss': 0.5849, 'grad_norm': 2.3223671913146973, 'learning_rate': 0.00042871385842472583, 'epoch': 0.59}\n",
      "{'loss': 0.5456, 'grad_norm': 3.0798351764678955, 'learning_rate': 0.0004037886340977069, 'epoch': 0.62}\n",
      "{'loss': 0.5609, 'grad_norm': 2.367668867111206, 'learning_rate': 0.00037886340977068793, 'epoch': 0.64}\n",
      "{'loss': 0.4812, 'grad_norm': 4.103032112121582, 'learning_rate': 0.00035393818544366903, 'epoch': 0.66}\n",
      "{'loss': 0.5779, 'grad_norm': 1.9440083503723145, 'learning_rate': 0.0003290129611166501, 'epoch': 0.69}\n",
      "{'loss': 0.5076, 'grad_norm': 1.978494644165039, 'learning_rate': 0.0003040877367896311, 'epoch': 0.71}\n",
      "{'loss': 0.457, 'grad_norm': 1.8103049993515015, 'learning_rate': 0.0002791625124626122, 'epoch': 0.74}\n",
      "{'loss': 0.4453, 'grad_norm': 1.9544771909713745, 'learning_rate': 0.0002542372881355932, 'epoch': 0.76}\n",
      "{'loss': 0.4794, 'grad_norm': 2.432069778442383, 'learning_rate': 0.00022931206380857427, 'epoch': 0.78}\n",
      "{'loss': 0.4701, 'grad_norm': 2.1191565990448, 'learning_rate': 0.00020438683948155535, 'epoch': 0.81}\n",
      "{'loss': 0.4158, 'grad_norm': 3.4953062534332275, 'learning_rate': 0.0001794616151545364, 'epoch': 0.83}\n",
      "{'loss': 0.4509, 'grad_norm': 1.940826654434204, 'learning_rate': 0.00015453639082751744, 'epoch': 0.85}\n",
      "{'loss': 0.453, 'grad_norm': 1.4921139478683472, 'learning_rate': 0.00012961116650049852, 'epoch': 0.88}\n",
      "{'loss': 0.4119, 'grad_norm': 1.9985525608062744, 'learning_rate': 0.00010468594217347957, 'epoch': 0.9}\n",
      "{'loss': 0.4524, 'grad_norm': 2.0024688243865967, 'learning_rate': 7.976071784646061e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4099, 'grad_norm': 1.9158543348312378, 'learning_rate': 5.4835493519441675e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3385, 'grad_norm': 1.582151174545288, 'learning_rate': 2.991026919242273e-05, 'epoch': 0.97}\n",
      "{'loss': 0.4315, 'grad_norm': 2.152574300765991, 'learning_rate': 4.985044865403788e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f93c6f963a4a5fa48c769f4e8646fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34885716438293457, 'eval_runtime': 4764.7537, 'eval_samples_per_second': 1.173, 'eval_steps_per_second': 0.147, 'epoch': 1.0}\n",
      "{'train_runtime': 31562.1048, 'train_samples_per_second': 0.267, 'train_steps_per_second': 0.033, 'train_loss': 0.6174209246947895, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1053, training_loss=0.6174209246947895, metrics={'train_runtime': 31562.1048, 'train_samples_per_second': 0.267, 'train_steps_per_second': 0.033, 'total_flos': 2.7032376545496e+18, 'train_loss': 0.6174209246947895, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp code to train my my local machine\n",
    "# common_voice[\"test\"]\n",
    "subset = common_voice[\"test\"].select(range(1000))\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]d:\\Documents\\Github\\adapters\\src\\adapters\\models\\whisper\\modeling_whisper.py:380: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "d:\\Anaconda\\envs\\adapters_env\\Lib\\site-packages\\transformers\\generation\\utils.py:876: FutureWarning: You have explicitly specified `forced_decoder_ids`. This functionality has been deprecated and will throw an error in v4.40. Please remove the `forced_decoder_ids` argument in favour of `input_ids` or `decoder_input_ids` respectively.\n",
      "  warnings.warn(\n",
      "100%|██████████| 13/13 [01:13<00:00,  5.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=3267.0\n"
     ]
    }
   ],
   "source": [
    "#model inference\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "########### replace with common_voice['test']\n",
    "eval_dataloader = DataLoader(subset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    decoder_input_ids=batch[\"labels\"][:, :4].to(\"cuda\"),\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            metric.add_batch(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "            )\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * metric.compute()\n",
    "print(f\"{wer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to save your model and or publish to huggingface, sign into the huggingface_hub via the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420e3b187243408fa432045549695ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always save your model by using the `save_adapter` function locally onto your computer or you can directly upload them to the ``HuggingFace`` hub. Make sure you include the parameter `adapterhub_tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the model\n",
    "save_directory = \"./my_model_directory\"\n",
    "# Save the model\n",
    "model.save_adapter(save_directory, \"whisper_adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_adapter_to_hub(\n",
    "    \"whisper\",\n",
    "    \"whisper_adapter\",\n",
    "    adapterhub_tag=\"seq2seq/whisper\",\n",
    "    datasets_tag=\"mozilla-foundation/common_voice_11_0\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapters_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
