{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd72df66",
   "metadata": {},
   "source": [
    "# Finetuning Llama 2 with _Adapters_ and QLoRA\n",
    "\n",
    "In this notebook, we show how to efficiently fine-tune a quantized 7B Llama 2 model using [**QLoRA** (Dettmers et al., 2023)](https://arxiv.org/abs/2305.14314) and the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) library.\n",
    "\n",
    "Specifically, we finetune Llama 2 on a supervised **instruction tuning** dataset collected by the [Open Assistant project](https://github.com/LAION-AI/Open-Assistant) for training chatbot models. This is similar to the setup used to train the Guanaco models in the QLoRA paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa993e5",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Besides `adapters`, we require `bitsandbytes` for quantization and `accelerate` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7bec93-c233-4493-b5d7-23d06f02d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qq -U adapters accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad77898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64aabb0-25ef-455f-9494-b6f4ca3ecfc9",
   "metadata": {},
   "source": [
    "## Load Open Assistant dataset\n",
    "\n",
    "We use the [`timdettmers/openassistant-guanaco`](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset by the QLoRA, which contains a small subset of conversations from the full Open Assistant database and was also used to finetune the Guanaco models in the QLoRA paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe883fe8-868c-4cc8-92e5-ed9889143ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/adapters/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2c4bf",
   "metadata": {},
   "source": [
    "Our training dataset has roughly 10k training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aab5773-415c-4c90-904a-f5c0a755abfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 9846\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 518\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b666c7-108b-4a64-aaec-64dfb1b10078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851ab41-4a7b-4df1-8140-48623daeae99",
   "metadata": {},
   "source": [
    "## Load and prepare model and tokenizer\n",
    "\n",
    "We download the the official Llama 2 7B checkpoints from the HuggingFace Hub (**Note:** You must request access to this model on the HuggingFace website and use an API token to download it.).\n",
    "\n",
    "Via the `BitsAndBytesConfig`, we specify that the model should be loaded in 4bit quantization and with double quantization for even better memory efficiency. See [their documentation](https://huggingface.co/docs/bitsandbytes/main/en/index) for more on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ef7f26-f87b-4c54-924f-c9661bc1bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/adapters/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/envs/adapters/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "modelpath=\"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Load 4-bit quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,    \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    ),\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f69bb",
   "metadata": {},
   "source": [
    "We initialize the adapter functionality in the loaded model via `adapters.init()` and add a new LoRA adapter (named `\"assistant_adapter\"`) via `add_adapter()`.\n",
    "\n",
    "In the call to `LoRAConfig()`, you can configure how and where LoRA layers are added to the model. Here, we want to add LoRA layers to all linear projections of the self-attention modules (`attn_matrices=[\"q\", \"k\", \"v\"]`) as well as intermediate and outputa linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6897387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "assistant_adapter        lora            112,197,632       3.330       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                              3,369,340,928     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import adapters\n",
    "from adapters import LoRAConfig\n",
    "\n",
    "adapters.init(model)\n",
    "\n",
    "config = LoRAConfig(\n",
    "    selfattn_lora=True, intermediate_lora=True, output_lora=True,\n",
    "    attn_matrices=[\"q\", \"k\", \"v\"],\n",
    "    alpha=16, r=64, dropout=0.1\n",
    ")\n",
    "model.add_adapter(\"assistant_adapter\", config=config)\n",
    "model.train_adapter(\"assistant_adapter\")\n",
    "\n",
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7211c68-07a3-4d24-a7af-a3691063a758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayerWithAdapters(\n",
       "        (self_attn): LlamaSdpaAttentionWithAdapters(\n",
       "          (q_proj): LoRALinear4bit(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (loras): ModuleDict(\n",
       "              (assistant_adapter): LoRA(\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (k_proj): LoRALinear4bit(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (loras): ModuleDict(\n",
       "              (assistant_adapter): LoRA(\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (v_proj): LoRALinear4bit(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (loras): ModuleDict(\n",
       "              (assistant_adapter): LoRA(\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (prefix_tuning): PrefixTuningLayer(\n",
       "            (prefix_gates): ModuleDict()\n",
       "            (pool): PrefixTuningPool(\n",
       "              (prefix_tunings): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): LoRALinear4bit(\n",
       "            in_features=4096, out_features=11008, bias=False\n",
       "            (loras): ModuleDict(\n",
       "              (assistant_adapter): LoRA(\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (down_proj): LoRALinear4bit(\n",
       "            in_features=11008, out_features=4096, bias=False\n",
       "            (loras): ModuleDict(\n",
       "              (assistant_adapter): LoRA(\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (attention_adapters): BottleneckLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): BottleneckLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24289126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 374607872 0.10369450727620085\n",
      "torch.uint8 3238002688 0.8963054927237991\n"
     ]
    }
   ],
   "source": [
    "# Verifying the datatypes.\n",
    "dtypes = {}\n",
    "for _, p in model.named_parameters():\n",
    "    dtype = p.dtype\n",
    "    if dtype not in dtypes:\n",
    "        dtypes[dtype] = 0\n",
    "    dtypes[dtype] += p.numel()\n",
    "total = 0\n",
    "for k, v in dtypes.items():\n",
    "    total += v\n",
    "for k, v in dtypes.items():\n",
    "    print(k, v, v / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913bc18-8a26-4a1b-8abc-3bc8e672f191",
   "metadata": {},
   "source": [
    "## Prepare data for training\n",
    "\n",
    "The dataset is tokenized and truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a941dc-9e9e-4bbd-9c2a-a54f2fd72071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|██████████| 9846/9846 [00:01<00:00, 9248.73 examples/s] \n",
      "Map (num_proc=24): 100%|██████████| 518/518 [00:00<00:00, 1767.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "def tokenize(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512, # can set to longer values such as 2048\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    num_proc=os.cpu_count(),    # multithreaded\n",
    "    remove_columns=[\"text\"]     # don't need this anymore, we have tokens from here on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c427e534-2214-4bc6-8c73-4a81c4984db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 9846\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 518\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27287f0e-56f0-458f-8c2e-9124a9739d48",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We specify training hyperparameters and train the model using the `AdapterTrainer` class.\n",
    "\n",
    "These hyperparameters are not well tuned, so feel free to play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36b9ad9-d0c0-4dc7-a4c8-03765891dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=0.0002,\n",
    "    group_by_length=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d8b96-bcfe-490d-8462-dfb44d575432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AdapterTrainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe958104",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Finally, we can prompt the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "from transformers import logging\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "def prompt_model(model, text: str):\n",
    "    batch = tokenizer(f\"### Human: {text}\\n### Assistant:\", return_tensors=\"pt\")\n",
    "    batch = batch.to(model.device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_model(model, \"Explain Calculus to a primary school student\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb96af2",
   "metadata": {},
   "source": [
    "## Merge LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5176ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.merge_adapter(\"assistant_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_model(model, \"Explain NLP in simple terms\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
