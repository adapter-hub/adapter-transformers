{"format": "torch", "nodes": [{"name": "bert", "id": 139967203802976, "class_name": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (attention_text_task_adapters): ModuleDict()\n            (adapter_fusion_layer): ModuleDict()\n            (attention_text_lang_adapters): ModuleDict()\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (adapter_fusion_layer): ModuleDict()\n          (layer_text_task_adapters): ModuleDict(\n            (mnli): Adapter(\n              (non_linearity): Activation_Function_Class()\n              (adapter_down): Sequential(\n                (0): Linear(in_features=768, out_features=48, bias=True)\n                (1): Activation_Function_Class()\n              )\n              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n            )\n          )\n          (layer_text_lang_adapters): ModuleDict()\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n  (invertible_lang_adapters): ModuleDict()\n)", "parameters": [["embeddings.word_embeddings.weight", [28996, 768]], ["embeddings.position_embeddings.weight", [512, 768]], ["embeddings.token_type_embeddings.weight", [2, 768]], ["embeddings.LayerNorm.weight", [768]], ["embeddings.LayerNorm.bias", [768]], ["encoder.layer.0.attention.self.query.weight", [768, 768]], ["encoder.layer.0.attention.self.query.bias", [768]], ["encoder.layer.0.attention.self.key.weight", [768, 768]], ["encoder.layer.0.attention.self.key.bias", [768]], ["encoder.layer.0.attention.self.value.weight", [768, 768]], ["encoder.layer.0.attention.self.value.bias", [768]], ["encoder.layer.0.attention.output.dense.weight", [768, 768]], ["encoder.layer.0.attention.output.dense.bias", [768]], ["encoder.layer.0.attention.output.LayerNorm.weight", [768]], ["encoder.layer.0.attention.output.LayerNorm.bias", [768]], ["encoder.layer.0.intermediate.dense.weight", [3072, 768]], ["encoder.layer.0.intermediate.dense.bias", [3072]], ["encoder.layer.0.output.dense.weight", [768, 3072]], ["encoder.layer.0.output.dense.bias", [768]], ["encoder.layer.0.output.LayerNorm.weight", [768]], ["encoder.layer.0.output.LayerNorm.bias", [768]], ["encoder.layer.0.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.0.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.0.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.0.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.1.attention.self.query.weight", [768, 768]], ["encoder.layer.1.attention.self.query.bias", [768]], ["encoder.layer.1.attention.self.key.weight", [768, 768]], ["encoder.layer.1.attention.self.key.bias", [768]], ["encoder.layer.1.attention.self.value.weight", [768, 768]], ["encoder.layer.1.attention.self.value.bias", [768]], ["encoder.layer.1.attention.output.dense.weight", [768, 768]], ["encoder.layer.1.attention.output.dense.bias", [768]], ["encoder.layer.1.attention.output.LayerNorm.weight", [768]], ["encoder.layer.1.attention.output.LayerNorm.bias", [768]], ["encoder.layer.1.intermediate.dense.weight", [3072, 768]], ["encoder.layer.1.intermediate.dense.bias", [3072]], ["encoder.layer.1.output.dense.weight", [768, 3072]], ["encoder.layer.1.output.dense.bias", [768]], ["encoder.layer.1.output.LayerNorm.weight", [768]], ["encoder.layer.1.output.LayerNorm.bias", [768]], ["encoder.layer.1.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.1.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.1.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.1.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.2.attention.self.query.weight", [768, 768]], ["encoder.layer.2.attention.self.query.bias", [768]], ["encoder.layer.2.attention.self.key.weight", [768, 768]], ["encoder.layer.2.attention.self.key.bias", [768]], ["encoder.layer.2.attention.self.value.weight", [768, 768]], ["encoder.layer.2.attention.self.value.bias", [768]], ["encoder.layer.2.attention.output.dense.weight", [768, 768]], ["encoder.layer.2.attention.output.dense.bias", [768]], ["encoder.layer.2.attention.output.LayerNorm.weight", [768]], ["encoder.layer.2.attention.output.LayerNorm.bias", [768]], ["encoder.layer.2.intermediate.dense.weight", [3072, 768]], ["encoder.layer.2.intermediate.dense.bias", [3072]], ["encoder.layer.2.output.dense.weight", [768, 3072]], ["encoder.layer.2.output.dense.bias", [768]], ["encoder.layer.2.output.LayerNorm.weight", [768]], ["encoder.layer.2.output.LayerNorm.bias", [768]], ["encoder.layer.2.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.2.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.2.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.2.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.3.attention.self.query.weight", [768, 768]], ["encoder.layer.3.attention.self.query.bias", [768]], ["encoder.layer.3.attention.self.key.weight", [768, 768]], ["encoder.layer.3.attention.self.key.bias", [768]], ["encoder.layer.3.attention.self.value.weight", [768, 768]], ["encoder.layer.3.attention.self.value.bias", [768]], ["encoder.layer.3.attention.output.dense.weight", [768, 768]], ["encoder.layer.3.attention.output.dense.bias", [768]], ["encoder.layer.3.attention.output.LayerNorm.weight", [768]], ["encoder.layer.3.attention.output.LayerNorm.bias", [768]], ["encoder.layer.3.intermediate.dense.weight", [3072, 768]], ["encoder.layer.3.intermediate.dense.bias", [3072]], ["encoder.layer.3.output.dense.weight", [768, 3072]], ["encoder.layer.3.output.dense.bias", [768]], ["encoder.layer.3.output.LayerNorm.weight", [768]], ["encoder.layer.3.output.LayerNorm.bias", [768]], ["encoder.layer.3.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.3.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.3.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.3.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.4.attention.self.query.weight", [768, 768]], ["encoder.layer.4.attention.self.query.bias", [768]], ["encoder.layer.4.attention.self.key.weight", [768, 768]], ["encoder.layer.4.attention.self.key.bias", [768]], ["encoder.layer.4.attention.self.value.weight", [768, 768]], ["encoder.layer.4.attention.self.value.bias", [768]], ["encoder.layer.4.attention.output.dense.weight", [768, 768]], ["encoder.layer.4.attention.output.dense.bias", [768]], ["encoder.layer.4.attention.output.LayerNorm.weight", [768]], ["encoder.layer.4.attention.output.LayerNorm.bias", [768]], ["encoder.layer.4.intermediate.dense.weight", [3072, 768]], ["encoder.layer.4.intermediate.dense.bias", [3072]], ["encoder.layer.4.output.dense.weight", [768, 3072]], ["encoder.layer.4.output.dense.bias", [768]], ["encoder.layer.4.output.LayerNorm.weight", [768]], ["encoder.layer.4.output.LayerNorm.bias", [768]], ["encoder.layer.4.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.4.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.4.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.4.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.5.attention.self.query.weight", [768, 768]], ["encoder.layer.5.attention.self.query.bias", [768]], ["encoder.layer.5.attention.self.key.weight", [768, 768]], ["encoder.layer.5.attention.self.key.bias", [768]], ["encoder.layer.5.attention.self.value.weight", [768, 768]], ["encoder.layer.5.attention.self.value.bias", [768]], ["encoder.layer.5.attention.output.dense.weight", [768, 768]], ["encoder.layer.5.attention.output.dense.bias", [768]], ["encoder.layer.5.attention.output.LayerNorm.weight", [768]], ["encoder.layer.5.attention.output.LayerNorm.bias", [768]], ["encoder.layer.5.intermediate.dense.weight", [3072, 768]], ["encoder.layer.5.intermediate.dense.bias", [3072]], ["encoder.layer.5.output.dense.weight", [768, 3072]], ["encoder.layer.5.output.dense.bias", [768]], ["encoder.layer.5.output.LayerNorm.weight", [768]], ["encoder.layer.5.output.LayerNorm.bias", [768]], ["encoder.layer.5.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.5.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.5.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.5.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.6.attention.self.query.weight", [768, 768]], ["encoder.layer.6.attention.self.query.bias", [768]], ["encoder.layer.6.attention.self.key.weight", [768, 768]], ["encoder.layer.6.attention.self.key.bias", [768]], ["encoder.layer.6.attention.self.value.weight", [768, 768]], ["encoder.layer.6.attention.self.value.bias", [768]], ["encoder.layer.6.attention.output.dense.weight", [768, 768]], ["encoder.layer.6.attention.output.dense.bias", [768]], ["encoder.layer.6.attention.output.LayerNorm.weight", [768]], ["encoder.layer.6.attention.output.LayerNorm.bias", [768]], ["encoder.layer.6.intermediate.dense.weight", [3072, 768]], ["encoder.layer.6.intermediate.dense.bias", [3072]], ["encoder.layer.6.output.dense.weight", [768, 3072]], ["encoder.layer.6.output.dense.bias", [768]], ["encoder.layer.6.output.LayerNorm.weight", [768]], ["encoder.layer.6.output.LayerNorm.bias", [768]], ["encoder.layer.6.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.6.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.6.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.6.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.7.attention.self.query.weight", [768, 768]], ["encoder.layer.7.attention.self.query.bias", [768]], ["encoder.layer.7.attention.self.key.weight", [768, 768]], ["encoder.layer.7.attention.self.key.bias", [768]], ["encoder.layer.7.attention.self.value.weight", [768, 768]], ["encoder.layer.7.attention.self.value.bias", [768]], ["encoder.layer.7.attention.output.dense.weight", [768, 768]], ["encoder.layer.7.attention.output.dense.bias", [768]], ["encoder.layer.7.attention.output.LayerNorm.weight", [768]], ["encoder.layer.7.attention.output.LayerNorm.bias", [768]], ["encoder.layer.7.intermediate.dense.weight", [3072, 768]], ["encoder.layer.7.intermediate.dense.bias", [3072]], ["encoder.layer.7.output.dense.weight", [768, 3072]], ["encoder.layer.7.output.dense.bias", [768]], ["encoder.layer.7.output.LayerNorm.weight", [768]], ["encoder.layer.7.output.LayerNorm.bias", [768]], ["encoder.layer.7.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.7.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.7.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.7.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.8.attention.self.query.weight", [768, 768]], ["encoder.layer.8.attention.self.query.bias", [768]], ["encoder.layer.8.attention.self.key.weight", [768, 768]], ["encoder.layer.8.attention.self.key.bias", [768]], ["encoder.layer.8.attention.self.value.weight", [768, 768]], ["encoder.layer.8.attention.self.value.bias", [768]], ["encoder.layer.8.attention.output.dense.weight", [768, 768]], ["encoder.layer.8.attention.output.dense.bias", [768]], ["encoder.layer.8.attention.output.LayerNorm.weight", [768]], ["encoder.layer.8.attention.output.LayerNorm.bias", [768]], ["encoder.layer.8.intermediate.dense.weight", [3072, 768]], ["encoder.layer.8.intermediate.dense.bias", [3072]], ["encoder.layer.8.output.dense.weight", [768, 3072]], ["encoder.layer.8.output.dense.bias", [768]], ["encoder.layer.8.output.LayerNorm.weight", [768]], ["encoder.layer.8.output.LayerNorm.bias", [768]], ["encoder.layer.8.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.8.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.8.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.8.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.9.attention.self.query.weight", [768, 768]], ["encoder.layer.9.attention.self.query.bias", [768]], ["encoder.layer.9.attention.self.key.weight", [768, 768]], ["encoder.layer.9.attention.self.key.bias", [768]], ["encoder.layer.9.attention.self.value.weight", [768, 768]], ["encoder.layer.9.attention.self.value.bias", [768]], ["encoder.layer.9.attention.output.dense.weight", [768, 768]], ["encoder.layer.9.attention.output.dense.bias", [768]], ["encoder.layer.9.attention.output.LayerNorm.weight", [768]], ["encoder.layer.9.attention.output.LayerNorm.bias", [768]], ["encoder.layer.9.intermediate.dense.weight", [3072, 768]], ["encoder.layer.9.intermediate.dense.bias", [3072]], ["encoder.layer.9.output.dense.weight", [768, 3072]], ["encoder.layer.9.output.dense.bias", [768]], ["encoder.layer.9.output.LayerNorm.weight", [768]], ["encoder.layer.9.output.LayerNorm.bias", [768]], ["encoder.layer.9.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.9.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.9.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.9.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.10.attention.self.query.weight", [768, 768]], ["encoder.layer.10.attention.self.query.bias", [768]], ["encoder.layer.10.attention.self.key.weight", [768, 768]], ["encoder.layer.10.attention.self.key.bias", [768]], ["encoder.layer.10.attention.self.value.weight", [768, 768]], ["encoder.layer.10.attention.self.value.bias", [768]], ["encoder.layer.10.attention.output.dense.weight", [768, 768]], ["encoder.layer.10.attention.output.dense.bias", [768]], ["encoder.layer.10.attention.output.LayerNorm.weight", [768]], ["encoder.layer.10.attention.output.LayerNorm.bias", [768]], ["encoder.layer.10.intermediate.dense.weight", [3072, 768]], ["encoder.layer.10.intermediate.dense.bias", [3072]], ["encoder.layer.10.output.dense.weight", [768, 3072]], ["encoder.layer.10.output.dense.bias", [768]], ["encoder.layer.10.output.LayerNorm.weight", [768]], ["encoder.layer.10.output.LayerNorm.bias", [768]], ["encoder.layer.10.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.10.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.10.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.10.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["encoder.layer.11.attention.self.query.weight", [768, 768]], ["encoder.layer.11.attention.self.query.bias", [768]], ["encoder.layer.11.attention.self.key.weight", [768, 768]], ["encoder.layer.11.attention.self.key.bias", [768]], ["encoder.layer.11.attention.self.value.weight", [768, 768]], ["encoder.layer.11.attention.self.value.bias", [768]], ["encoder.layer.11.attention.output.dense.weight", [768, 768]], ["encoder.layer.11.attention.output.dense.bias", [768]], ["encoder.layer.11.attention.output.LayerNorm.weight", [768]], ["encoder.layer.11.attention.output.LayerNorm.bias", [768]], ["encoder.layer.11.intermediate.dense.weight", [3072, 768]], ["encoder.layer.11.intermediate.dense.bias", [3072]], ["encoder.layer.11.output.dense.weight", [768, 3072]], ["encoder.layer.11.output.dense.bias", [768]], ["encoder.layer.11.output.LayerNorm.weight", [768]], ["encoder.layer.11.output.LayerNorm.bias", [768]], ["encoder.layer.11.output.layer_text_task_adapters.mnli.adapter_down.0.weight", [48, 768]], ["encoder.layer.11.output.layer_text_task_adapters.mnli.adapter_down.0.bias", [48]], ["encoder.layer.11.output.layer_text_task_adapters.mnli.adapter_up.weight", [768, 48]], ["encoder.layer.11.output.layer_text_task_adapters.mnli.adapter_up.bias", [768]], ["pooler.dense.weight", [768, 768]], ["pooler.dense.bias", [768]]], "output_shape": [[64, 128, 768], [64, 768]], "num_parameters": [22268928, 393216, 1536, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 36864, 48, 36864, 768, 589824, 768]}, {"name": "heads.mnli.0", "id": 139966801253936, "class_name": "Dropout(p=0.1, inplace=False)", "parameters": [], "output_shape": [[64, 768]], "num_parameters": []}, {"name": "heads.mnli.1", "id": 139966801253992, "class_name": "Linear(in_features=768, out_features=768, bias=True)", "parameters": [["weight", [768, 768]], ["bias", [768]]], "output_shape": [[64, 768]], "num_parameters": [589824, 768]}, {"name": "heads.mnli.2", "id": 139966801254272, "class_name": "Activation_Function_Class()", "parameters": [], "output_shape": [[64, 768]], "num_parameters": []}, {"name": "heads.mnli.3", "id": 139966801254216, "class_name": "Dropout(p=0.1, inplace=False)", "parameters": [], "output_shape": [[64, 768]], "num_parameters": []}, {"name": "heads.mnli.4", "id": 139966801254160, "class_name": "Linear(in_features=768, out_features=3, bias=True)", "parameters": [["weight", [3, 768]], ["bias", [3]]], "output_shape": [[64, 3]], "num_parameters": [2304, 3]}], "edges": []}