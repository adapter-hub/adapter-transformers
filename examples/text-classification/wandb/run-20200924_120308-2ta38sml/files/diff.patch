diff --git a/examples/text-classification/run_glue_wh.sh b/examples/text-classification/run_glue_wh.sh
index 728bba50..40630365 100644
--- a/examples/text-classification/run_glue_wh.sh
+++ b/examples/text-classification/run_glue_wh.sh
@@ -1,5 +1,6 @@
 # REQUIRED: DOWNLOAD SST FROME GLUE $ python3 transformers/utils/download_glue_data.py --tasks MNLI
-export GLUE_DIR="/content/gdrive/My Drive/adapter/data/glue_data"
+cd /home/theorist17/projects/adapter/adapter-transformers/
+export GLUE_DIR="/home/theorist17/projects/adapter/data/glue_data"
 export TASK_NAME=MNLI
 
 python3 run_glue_wh.py \
@@ -12,9 +13,9 @@ python3 run_glue_wh.py \
   --per_device_train_batch_size 64 \
   --learning_rate 1e-4 \
   --num_train_epochs 10.0 \
-  --output_dir "/content/gdrive/My Drive/adapter/adapters/$TASK_NAME" \
+  --output_dir "/home/theorist17/projects/adapter/adapters/$TASK_NAME" \
   --overwrite_output_dir \
   --train_adapter \
-  --adapter_config pfeiffer
-
-#  --output_dir /tmp/$TASK_NAME \
\ No newline at end of file
+  --adapter_config pfeiffer \
+  --logging_steps 1000 \
+  --evaluate_during_training
\ No newline at end of file
diff --git a/run_conceptnet.py b/run_conceptnet.py
index 29036c43..f34432ff 100644
--- a/run_conceptnet.py
+++ b/run_conceptnet.py
@@ -572,6 +572,7 @@ def main():
     model.add_classification_head(args.task_name, num_labels=num_labels)
     # Setup adapters
     setup_task_adapter_training(model, args.task_name, adapter_args)
+    
 
     if args.local_rank == 0:
         torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
diff --git a/run_conceptnet.sh b/run_conceptnet.sh
index a51fb772..404b4ee3 100644
--- a/run_conceptnet.sh
+++ b/run_conceptnet.sh
@@ -8,17 +8,17 @@ python3 run_conceptnet.py \
   --do_train \
   --do_eval \
   --data_dir "$CONCEPTNET_DIR" \
-  --max_seq_length 64 \
-  --per_gpu_train_batch_size 128 \
-  --per_gpu_eval_batch_size 128 \
+  --max_seq_length 128 \
+  --per_gpu_train_batch_size 256 \
+  --per_gpu_eval_batch_size 256 \
   --cache_dir "/home/theorist17/projects/adapter/adapters/$TASK_NAME" \
-# --evaluate_during_training \
   --do_lower_case \
   --logging_steps 2000 \
   --save_steps 2000 \
   --output_dir "/home/theorist17/projects/adapter/adapters/$TASK_NAME" \
   --overwrite_output_dir \
-  --num_train_epochs 3 \
+  --num_train_epochs 3
+  # --evaluate_during_training \
   #--overwrite_cache # features!
   #--train_adapter \
   #--adapter_config pfeiffer
diff --git a/run_cqa_wh.py b/run_cqa_wh.py
index bc4c5a6d..d9d10dd1 100644
--- a/run_cqa_wh.py
+++ b/run_cqa_wh.py
@@ -34,7 +34,9 @@ from transformers import (
     TrainingArguments,
     set_seed,
     setup_task_adapter_training,
+    AdapterType
 )
+from transformers.modeling_bert import BertForMultipleChoice
 from utils_multiple_choice import MultipleChoiceDataset, Split, processors
 
 
@@ -145,15 +147,17 @@ def main():
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
         cache_dir=model_args.cache_dir,
     )
-    model = AutoModelForMultipleChoice.from_pretrained(
+    model = BertForMultipleChoice.from_pretrained(
         model_args.model_name_or_path,
         from_tf=bool(".ckpt" in model_args.model_name_or_path),
         config=config,
         cache_dir=model_args.cache_dir,
     )
-
+    
     # Setup adapters
     task_name = data_args.task_name
+    adapter_args.train_adapter = True
+    #model.add_classification_head(data_args.task_name, num_labels=num_labels)
     setup_task_adapter_training(model, task_name, adapter_args)
 
     # Get datasets
@@ -185,7 +189,7 @@ def main():
     def compute_metrics(p: EvalPrediction) -> Dict:
         preds = np.argmax(p.predictions, axis=1)
         return {"acc": simple_accuracy(preds, p.label_ids)}
-
+    
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
diff --git a/run_cqa_wh.sh b/run_cqa_wh.sh
index 67f404c0..c6f1786c 100644
--- a/run_cqa_wh.sh
+++ b/run_cqa_wh.sh
@@ -3,16 +3,19 @@ export DATA_DIR="/home/theorist17/projects/adapter/data"
 export TASK_NAME=commonsenseqa
 
 python3 run_cqa_wh.py \
---task_name $TASK_NAME \
 --model_name_or_path bert-base-cased \
+--task_name $TASK_NAME \
 --do_train \
 --do_eval \
 --data_dir "$DATA_DIR/$TASK_NAME" \
+--max_seq_length 128 \
 --learning_rate 5e-5 \
---num_train_epochs 3 \
---max_seq_length 80 \
---output_dir "home/theorist17/projects/adapter/adapters/$TASK_NAME" \
+--logging_steps 100 \
+--save_steps 100 \
+--num_train_epochs 10 \
+--output_dir "/home/theorist17/projects/adapter/adapters/$TASK_NAME" \
 --per_gpu_eval_batch_size=16 \
 --per_device_train_batch_size=16 \
 --gradient_accumulation_steps 2 \
---overwrite_output
+--overwrite_output \
+--evaluate_during_training
\ No newline at end of file
diff --git a/run_fusion_cqa.py b/run_fusion_cqa.py
index 0cc885d4..c984a02e 100644
--- a/run_fusion_cqa.py
+++ b/run_fusion_cqa.py
@@ -29,7 +29,8 @@ import numpy as np
 from transformers import (  # setup_task_adapter_training,
     AdapterArguments,
     AutoConfig,
-    AutoModelForSequenceClassification,
+    AdapterConfig,
+    AutoModelForMultipleChoice,
     AutoTokenizer,
     EvalPrediction,
     GlueDataset,
@@ -45,6 +46,7 @@ from transformers import (
     set_seed,
 )
 
+from utils_multiple_choice import MultipleChoiceDataset, Split, processors
 
 logger = logging.getLogger(__name__)
 
@@ -116,8 +118,9 @@ def main():
     set_seed(training_args.seed)
 
     try:
-        num_labels = glue_tasks_num_labels[data_args.task_name]
-        output_mode = glue_output_modes[data_args.task_name]
+        processor = processors[data_args.task_name]()
+        label_list = processor.get_labels()
+        num_labels = len(label_list)
     except KeyError:
         raise ValueError("Task not found: %s" % (data_args.task_name))
 
@@ -137,7 +140,7 @@ def main():
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
         cache_dir=model_args.cache_dir,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
+    model = AutoModelForMultipleChoice.from_pretrained(
         model_args.model_name_or_path,
         from_tf=bool(".ckpt" in model_args.model_name_or_path),
         config=config,
@@ -145,71 +148,61 @@ def main():
     )
 
     # Setup adapters
-    # task_name = data_args.task_name
-    # language = adapter_args.language
-    # setup_task_adapter_training(model, task_name, adapter_args)
     from transformers.adapter_config import AdapterType
 
     base_model = getattr(model, model.base_model_prefix, model)
     base_model.set_adapter_config(AdapterType.text_task, adapter_args.adapter_config)
 
     from transformers.adapter_config import PfeifferConfig
-
-    # from transformers.adapter_config import  HoulsbyConfig
-
-    model.load_adapter(
-        "sentiment/sst-2@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion"
-    )
-    model.load_adapter(
-        "nli/multinli@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion"
-    )
-    model.load_adapter("nli/rte@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion")
-    model.load_adapter("sts/mrpc@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion")
-    model.load_adapter("sts/qqp@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion")
-    model.load_adapter("comsense/cosmosqa@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("comsense/csqa@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("comsense/hellaswag@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("comsense/siqa@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("comsense/winogrande@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("nli/cb@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("nli/sick@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("nli/scitail@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("qa/boolq@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("sentiment/imdb@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-
+    model.load_adapter("/home/theorist17/projects/adapter/adapters/MNLI/checkpoint-1400/mnli", 
+                       "text_task", config=PfeifferConfig(), with_head=False)
+    model.load_adapter("/home/theorist17/projects/adapter/adapters/commonsenseqa/commonsenseqa",
+                       "text_task", config=PfeifferConfig(), with_head=False)
+    model.load_adapter("/home/theorist17/projects/adapter/adapters/conceptnet/conceptnet",
+                       "text_task", config=PfeifferConfig(), with_head=False)
     adapter_names = [
         [
-            "sst_glue",
-            "multinli",
-            "rte",
-            "mrpc",
-            "qqp",
-            "cosmosqa",
-            "csqa",
-            "hellaswag",
-            "socialiqa",
-            "winogrande",
-            "cb",
-            "sick",
-            "scitail",
-            "boolq",
-            "imdb",
+            "mnli",
+            "commonsenseqa",
+            "conceptnet"
         ]
     ]
 
     model.add_fusion(adapter_names[0], "static", {"regularization": False})
     model.base_model.train_fusion(adapter_names[0])
+    
     # Get datasets
-    train_dataset = GlueDataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
-    eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode="dev") if training_args.do_eval else None
-    test_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode="test") if training_args.do_predict else None
-
+    train_dataset = (
+        MultipleChoiceDataset(
+            data_dir=data_args.data_dir,
+            tokenizer=tokenizer,
+            task=data_args.task_name,
+            max_seq_length=data_args.max_seq_length,
+            overwrite_cache=data_args.overwrite_cache,
+            mode=Split.train,
+        )
+        if training_args.do_train
+        else None
+    )
+    eval_dataset = (
+        MultipleChoiceDataset(
+            data_dir=data_args.data_dir,
+            tokenizer=tokenizer,
+            task=data_args.task_name,
+            max_seq_length=data_args.max_seq_length,
+            overwrite_cache=data_args.overwrite_cache,
+            mode=Split.dev,
+        )
+        if training_args.do_eval
+        else None
+    )
+    
+    def simple_accuracy(preds, labels):
+        return (preds == labels).mean()
+    
     def compute_metrics(p: EvalPrediction) -> Dict:
-        if output_mode == "classification":
-            preds = np.argmax(p.predictions, axis=1)
-        elif output_mode == "regression":
-            preds = np.squeeze(p.predictions)
-        return glue_compute_metrics(data_args.task_name, preds, p.label_ids)
+        preds = np.argmax(p.predictions, axis=1)
+        return {"acc": simple_accuracy(preds, p.label_ids)}
 
     # Initialize our Trainer
     trainer = Trainer(
@@ -239,52 +232,43 @@ def main():
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
 
-        # Loop to handle MNLI double evaluation (matched, mis-matched)
-        eval_datasets = [eval_dataset]
-        if data_args.task_name == "mnli":
-            mnli_mm_data_args = dataclasses.replace(data_args, task_name="mnli-mm")
-            eval_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, mode="dev"))
-
-        for eval_dataset in eval_datasets:
-            eval_result = trainer.evaluate(eval_dataset=eval_dataset)
-
-            output_eval_file = os.path.join(
-                training_args.output_dir, f"eval_results_{eval_dataset.args.task_name}.txt"
-            )
-            if trainer.is_world_master():
-                with open(output_eval_file, "w") as writer:
-                    logger.info("***** Eval results {} *****".format(eval_dataset.args.task_name))
-                    for key, value in eval_result.items():
-                        logger.info("  %s = %s", key, value)
-                        writer.write("%s = %s\n" % (key, value))
-
-            eval_results.update(eval_result)
-
-    if training_args.do_predict:
-        logging.info("*** Test ***")
-        test_datasets = [test_dataset]
-        if data_args.task_name == "mnli":
-            mnli_mm_data_args = dataclasses.replace(data_args, task_name="mnli-mm")
-            test_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, mode="test"))
-
-        for test_dataset in test_datasets:
-            predictions = trainer.predict(test_dataset=test_dataset).predictions
-            if output_mode == "classification":
-                predictions = np.argmax(predictions, axis=1)
-
-            output_test_file = os.path.join(
-                training_args.output_dir, f"test_results_{test_dataset.args.task_name}.txt"
-            )
-            if trainer.is_world_master():
-                with open(output_test_file, "w") as writer:
-                    logger.info("***** Test results {} *****".format(test_dataset.args.task_name))
-                    writer.write("index\tprediction\n")
-                    for index, item in enumerate(predictions):
-                        if output_mode == "regression":
-                            writer.write("%d\t%3.3f\n" % (index, item))
-                        else:
-                            item = test_dataset.get_labels()[item]
-                            writer.write("%d\t%s\n" % (index, item))
+        result = trainer.evaluate()
+
+        output_eval_file = os.path.join(training_args.output_dir, "eval_results.txt")
+        if trainer.is_world_master():
+            with open(output_eval_file, "w") as writer:
+                logger.info("***** Eval results *****")
+                for key, value in result.items():
+                    logger.info("  %s = %s", key, value)
+                    writer.write("%s = %s\n" % (key, value))
+
+                eval_results.update(result)
+
+    # if training_args.do_predict:
+    #     logging.info("*** Test ***")
+    #     test_datasets = [test_dataset]
+    #     if data_args.task_name == "mnli":
+    #         mnli_mm_data_args = dataclasses.replace(data_args, task_name="mnli-mm")
+    #         test_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, mode="test"))
+
+    #     for test_dataset in test_datasets:
+    #         predictions = trainer.predict(test_dataset=test_dataset).predictions
+    #         if output_mode == "classification":
+    #             predictions = np.argmax(predictions, axis=1)
+
+    #         output_test_file = os.path.join(
+    #             training_args.output_dir, f"test_results_{test_dataset.args.task_name}.txt"
+    #         )
+    #         if trainer.is_world_master():
+    #             with open(output_test_file, "w") as writer:
+    #                 logger.info("***** Test results {} *****".format(test_dataset.args.task_name))
+    #                 writer.write("index\tprediction\n")
+    #                 for index, item in enumerate(predictions):
+    #                     if output_mode == "regression":
+    #                         writer.write("%d\t%3.3f\n" % (index, item))
+    #                     else:
+    #                         item = test_dataset.get_labels()[item]
+    #                         writer.write("%d\t%s\n" % (index, item))
     return eval_results
 
 
diff --git a/run_fusion_cqa.sh b/run_fusion_cqa.sh
index 4ce4cf0e..07081a77 100644
--- a/run_fusion_cqa.sh
+++ b/run_fusion_cqa.sh
@@ -1,6 +1,5 @@
-# REQUIRED: DOWNLOAD SST FROME GLUE $ python3 transformers/utils/download_glue_data.py --tasks MNLI
 cd "/home/theorist17/projects/adapter/adapter-transformers"
-export DATA_DIR="home/theorist17/projects/adapter/data"
+export DATA_DIR="/home/theorist17/projects/adapter/data"
 export TASK_NAME=commonsenseqa
 
 python3 run_fusion_cqa.py \
@@ -9,10 +8,13 @@ python3 run_fusion_cqa.py \
 --do_train \
 --do_eval \
 --data_dir "$DATA_DIR/$TASK_NAME" \
---max_seq_length 80 \
+--max_seq_length 128 \
 --learning_rate 5e-5 \
---num_train_epochs 3 \
---output_dir "home/theorist17/projects/adapter/fusions/$TASK_NAME" \
+--logging_steps 100 \
+--save_steps 100 \
+--evaluate_during_training \
+--num_train_epochs 10 \
+--output_dir "/home/theorist17/projects/adapter/fusions/$TASK_NAME/" \
 --per_gpu_eval_batch_size=16 \
 --per_device_train_batch_size=16 \
 --gradient_accumulation_steps 2 \
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index 137a0b7d..17703bc2 100644
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -204,10 +204,10 @@ class Trainer:
         self.compute_metrics = compute_metrics
         self.prediction_loss_only = prediction_loss_only
         self.optimizers = optimizers
-        if tb_writer is not None:
-            self.tb_writer = tb_writer
-        elif is_tensorboard_available() and self.is_world_master():
-            self.tb_writer = SummaryWriter(log_dir=self.args.logging_dir)
+        # if tb_writer is not None:
+        #     self.tb_writer = tb_writer
+        # elif is_tensorboard_available() and self.is_world_master():
+        #     self.tb_writer = SummaryWriter(log_dir=self.args.logging_dir)
         if not is_tensorboard_available():
             logger.warning(
                 "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it."
@@ -488,10 +488,6 @@ class Trainer:
                     len(epoch_iterator) <= self.args.gradient_accumulation_steps
                     and (step + 1) == len(epoch_iterator)
                 ):
-                    # if model.module:
-                    #     model.config = model.module.config
-                    #     model.config.adapter_fusion["regularization"] = model.module.config.adapter_fusion["regularization"]
-                    
                     # apply adapter fusion weight regularization on the value matrix
                     if hasattr(self.model.config, "adapter_fusion") and self.model.config.adapter_fusion["regularization"]:
                         fusion_reg_loss = get_fusion_regularization_loss(model)
