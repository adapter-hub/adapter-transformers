diff --git a/examples/text-classification/run_glue_wh.sh b/examples/text-classification/run_glue_wh.sh
index 728bba50..298ab327 100644
--- a/examples/text-classification/run_glue_wh.sh
+++ b/examples/text-classification/run_glue_wh.sh
@@ -1,5 +1,6 @@
 # REQUIRED: DOWNLOAD SST FROME GLUE $ python3 transformers/utils/download_glue_data.py --tasks MNLI
-export GLUE_DIR="/content/gdrive/My Drive/adapter/data/glue_data"
+cd /home/theorist17/projects/adapter/adapter-transformers/examples/text-classification/
+export GLUE_DIR="/home/theorist17/projects/adapter/data/glue_data"
 export TASK_NAME=MNLI
 
 python3 run_glue_wh.py \
@@ -12,9 +13,10 @@ python3 run_glue_wh.py \
   --per_device_train_batch_size 64 \
   --learning_rate 1e-4 \
   --num_train_epochs 10.0 \
-  --output_dir "/content/gdrive/My Drive/adapter/adapters/$TASK_NAME" \
+  --output_dir "/home/theorist17/projects/adapter/adapters/$TASK_NAME" \
   --overwrite_output_dir \
   --train_adapter \
-  --adapter_config pfeiffer
-
-#  --output_dir /tmp/$TASK_NAME \
\ No newline at end of file
+  --adapter_config pfeiffer \
+  --save_steps 200 \
+  --logging_steps 200 \
+  --evaluate_during_training
diff --git a/run_conceptnet.py b/run_conceptnet.py
new file mode 100644
index 00000000..f34432ff
--- /dev/null
+++ b/run_conceptnet.py
@@ -0,0 +1,693 @@
+# coding=utf-8
+# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
+# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+""" Finetuning multi-lingual models on XNLI (e.g. Bert, DistilBERT, XLM).
+    Adapted from `examples/text-classification/run_glue.py`"""
+
+
+import argparse
+import glob
+import logging
+import os
+import random
+
+import numpy as np
+import torch
+from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
+from torch.utils.data.distributed import DistributedSampler
+from tqdm import tqdm, trange
+
+from transformers import (
+    WEIGHTS_NAME,
+    AdamW,
+    AutoConfig,
+    AutoModelForSequenceClassification,
+    AutoModelWithHeads,
+    AutoTokenizer,
+    get_linear_schedule_with_warmup,
+    setup_task_adapter_training,
+    AdapterArguments,
+    Trainer,
+    ConceptnetDataset,
+    EvalPrediction
+)
+from transformers import conceptnet_convert_examples_to_features as convert_examples_to_features
+from transformers import conceptnet_compute_metrics
+from transformers import conceptnet_output_modes as output_modes
+from transformers import conceptnet_processors as processors
+
+from transformers.data.data_collator import DataCollator
+from typing import Any, Dict, List, NewType, Tuple
+
+# try:
+#     from torch.utils.tensorboard import SummaryWriter
+# except ImportError:
+#     from tensorboardX import SummaryWriter
+
+
+logger = logging.getLogger(__name__)
+
+
+def set_seed(args):
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    if args.n_gpu > 0:
+        torch.cuda.manual_seed_all(args.seed)
+
+
+# def train(args, train_dataset, model, tokenizer):
+#     """ Train the model """
+#     if args.local_rank in [-1, 0]:
+#         tb_writer = SummaryWriter()
+
+#     args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
+#     train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
+#     train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
+
+#     if args.max_steps > 0:
+#         t_total = args.max_steps
+#         args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1
+#     else:
+#         t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs
+
+#     # Prepare optimizer and schedule (linear warmup and decay)
+#     no_decay = ["bias", "LayerNorm.weight"]
+#     optimizer_grouped_parameters = [
+#         {
+#             "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
+#             "weight_decay": args.weight_decay,
+#         },
+#         {"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], "weight_decay": 0.0},
+#     ]
+#     optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
+#     scheduler = get_linear_schedule_with_warmup(
+#         optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total
+#     )
+
+#     # Check if saved optimizer or scheduler states exist
+#     if os.path.isfile(os.path.join(args.model_name_or_path, "optimizer.pt")) and os.path.isfile(
+#         os.path.join(args.model_name_or_path, "scheduler.pt")
+#     ):
+#         # Load in optimizer and scheduler states
+#         optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, "optimizer.pt")))
+#         scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, "scheduler.pt")))
+
+#     if args.fp16:
+#         try:
+#             from apex import amp
+#         except ImportError:
+#             raise ImportError("Please install apex from https://www.github.com/nvidia/apex to use fp16 training.")
+#         model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)
+
+#     # multi-gpu training (should be after apex fp16 initialization)
+#     if args.n_gpu > 1:
+#         model = torch.nn.DataParallel(model)
+
+#     # Distributed training (should be after apex fp16 initialization)
+#     if args.local_rank != -1:
+#         model = torch.nn.parallel.DistributedDataParallel(
+#             model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True
+#         )
+
+#     # Train!
+#     logger.info("***** Running training *****")
+#     logger.info("  Num examples = %d", len(train_dataset))
+#     logger.info("  Num Epochs = %d", args.num_train_epochs)
+#     logger.info("  Instantaneous batch size per GPU = %d", args.per_gpu_train_batch_size)
+#     logger.info(
+#         "  Total train batch size (w. parallel, distributed & accumulation) = %d",
+#         args.train_batch_size
+#         * args.gradient_accumulation_steps
+#         * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),
+#     )
+#     logger.info("  Gradient Accumulation steps = %d", args.gradient_accumulation_steps)
+#     logger.info("  Total optimization steps = %d", t_total)
+
+#     global_step = 0
+#     epochs_trained = 0
+#     steps_trained_in_current_epoch = 0
+#     # Check if continuing training from a checkpoint
+#     if os.path.exists(args.model_name_or_path):
+#         # set global_step to gobal_step of last saved checkpoint from model path
+#         global_step = int(args.model_name_or_path.split("-")[-1].split("/")[0])
+#         epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)
+#         steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)
+
+#         logger.info("  Continuing training from checkpoint, will skip to saved global_step")
+#         logger.info("  Continuing training from epoch %d", epochs_trained)
+#         logger.info("  Continuing training from global step %d", global_step)
+#         logger.info("  Will skip the first %d steps in the first epoch", steps_trained_in_current_epoch)
+
+#     tr_loss, logging_loss = 0.0, 0.0
+#     model.zero_grad()
+#     train_iterator = trange(
+#         epochs_trained, int(args.num_train_epochs), desc="Epoch", disable=args.local_rank not in [-1, 0]
+#     )
+#     set_seed(args)  # Added here for reproductibility
+#     for _ in train_iterator:
+#         epoch_iterator = tqdm(train_dataloader, desc="Iteration", disable=args.local_rank not in [-1, 0])
+#         for step, batch in enumerate(epoch_iterator):
+#             # Skip past any already trained steps if resuming training
+#             if steps_trained_in_current_epoch > 0:
+#                 steps_trained_in_current_epoch -= 1
+#                 continue
+
+#             model.train()
+#             batch = tuple(t.to(args.device) for t in batch)
+#             inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
+#             if args.model_type != "distilbert":
+#                 inputs["token_type_ids"] = (
+#                     batch[2] if args.model_type in ["bert"] else None
+#                 )  # XLM and DistilBERT don't use segment_ids
+#             outputs = model(**inputs)
+#             loss = outputs[0]  # model outputs are always tuple in transformers (see doc)
+
+#             if args.n_gpu > 1:
+#                 loss = loss.mean()  # mean() to average on multi-gpu parallel training
+#             if args.gradient_accumulation_steps > 1:
+#                 loss = loss / args.gradient_accumulation_steps
+
+#             if args.fp16:
+#                 with amp.scale_loss(loss, optimizer) as scaled_loss:
+#                     scaled_loss.backward()
+#             else:
+#                 loss.backward()
+
+#             tr_loss += loss.item()
+#             if (step + 1) % args.gradient_accumulation_steps == 0:
+#                 if args.fp16:
+#                     torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
+#                 else:
+#                     torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)
+
+#                 optimizer.step()
+#                 scheduler.step()  # Update learning rate schedule
+#                 model.zero_grad()
+#                 global_step += 1
+
+#                 if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:
+#                     # Log metrics
+#                     if (
+#                         args.local_rank == -1 and args.evaluate_during_training
+#                     ):  # Only evaluate when single GPU otherwise metrics may not average well
+#                         results = evaluate(args, model, tokenizer)
+#                         for key, value in results.items():
+#                             tb_writer.add_scalar("eval_{}".format(key), value, global_step)
+#                     tb_writer.add_scalar("lr", scheduler.get_lr()[0], global_step)
+#                     tb_writer.add_scalar("loss", (tr_loss - logging_loss) / args.logging_steps, global_step)
+#                     logging_loss = tr_loss
+
+#                 if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:
+#                     # Save model checkpoint
+#                     output_dir = os.path.join(args.output_dir, "checkpoint-{}".format(global_step))
+#                     if not os.path.exists(output_dir):
+#                         os.makedirs(output_dir)
+#                     model_to_save = (
+#                         model.module if hasattr(model, "module") else model
+#                     )  # Take care of distributed/parallel training
+#                     model_to_save.save_pretrained(output_dir)
+#                     tokenizer.save_pretrained(output_dir)
+
+#                     torch.save(args, os.path.join(output_dir, "training_args.bin"))
+#                     logger.info("Saving model checkpoint to %s", output_dir)
+
+#                     torch.save(optimizer.state_dict(), os.path.join(output_dir, "optimizer.pt"))
+#                     torch.save(scheduler.state_dict(), os.path.join(output_dir, "scheduler.pt"))
+#                     logger.info("Saving optimizer and scheduler states to %s", output_dir)
+
+#             if args.max_steps > 0 and global_step > args.max_steps:
+#                 epoch_iterator.close()
+#                 break
+#         if args.max_steps > 0 and global_step > args.max_steps:
+#             train_iterator.close()
+#             break
+
+#     if args.local_rank in [-1, 0]:
+#         tb_writer.close()
+
+#     return global_step, tr_loss / global_step
+
+
+# def evaluate(args, model, tokenizer, prefix=""):
+#     eval_task_names = (args.task_name,)
+#     eval_outputs_dirs = (args.output_dir,)
+
+#     results = {}
+#     for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):
+#         eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)
+
+#         if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:
+#             os.makedirs(eval_output_dir)
+
+#         args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)
+#         # Note that DistributedSampler samples randomly
+#         eval_sampler = SequentialSampler(eval_dataset)
+#         eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)
+
+#         # multi-gpu eval
+#         if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):
+#             model = torch.nn.DataParallel(model)
+
+#         # Eval!
+#         logger.info("***** Running evaluation {} *****".format(prefix))
+#         logger.info("  Num examples = %d", len(eval_dataset))
+#         logger.info("  Batch size = %d", args.eval_batch_size)
+#         eval_loss = 0.0
+#         nb_eval_steps = 0
+#         preds = None
+#         out_label_ids = None
+#         for batch in tqdm(eval_dataloader, desc="Evaluating"):
+#             model.eval()
+#             batch = tuple(t.to(args.device) for t in batch)
+
+#             with torch.no_grad():
+#                 inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
+#                 if args.model_type != "distilbert":
+#                     inputs["token_type_ids"] = (
+#                         batch[2] if args.model_type in ["bert"] else None
+#                     )  # XLM and DistilBERT don't use segment_ids
+#                 outputs = model(**inputs)
+#                 tmp_eval_loss, logits = outputs[:2]
+
+#                 eval_loss += tmp_eval_loss.mean().item()
+#             nb_eval_steps += 1
+#             if preds is None:
+#                 preds = logits.detach().cpu().numpy()
+#                 out_label_ids = inputs["labels"].detach().cpu().numpy()
+#             else:
+#                 preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
+#                 out_label_ids = np.append(out_label_ids, inputs["labels"].detach().cpu().numpy(), axis=0)
+
+#         eval_loss = eval_loss / nb_eval_steps
+#         if args.output_mode == "classification":
+#             preds = np.argmax(preds, axis=1)
+#         else:
+#             raise ValueError("No other `output_mode` for XNLI.")
+#         result = compute_metrics(eval_task, preds, out_label_ids)
+#         results.update(result)
+
+#         output_eval_file = os.path.join(eval_output_dir, prefix, "eval_results.txt")
+#         with open(output_eval_file, "w") as writer:
+#             logger.info("***** Eval results {} *****".format(prefix))
+#             for key in sorted(result.keys()):
+#                 logger.info("  %s = %s", key, str(result[key]))
+#                 writer.write("%s = %s\n" % (key, str(result[key])))
+
+#     return results
+
+
+def load_and_cache_examples(args, task, tokenizer, evaluate=False):
+    if args.local_rank not in [-1, 0] and not evaluate:
+        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache
+
+    processor = processors[task]()
+    output_mode = output_modes[task]
+    # Load data features from cache or dataset file
+    cached_features_file = os.path.join(
+        args.data_dir,
+        "cached_{}_{}_{}_{}".format(
+            "test" if evaluate else "train",
+            list(filter(None, args.model_name_or_path.split("/"))).pop(),
+            str(args.max_seq_length),
+            str(task),
+        ),
+    )
+    if os.path.exists(cached_features_file) and not args.overwrite_cache:
+        logger.info("Loading features from cached file %s", cached_features_file)
+        features = torch.load(cached_features_file)
+    else:
+        logger.info("Creating features from dataset file at %s", args.data_dir)
+        label_list = processor.get_labels()
+        examples = (
+            processor.get_test_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)
+        )
+        features = convert_examples_to_features(
+            examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=output_mode,
+        )
+        if args.local_rank in [-1, 0]:
+            logger.info("Saving features into cached file %s", cached_features_file)
+            torch.save(features, cached_features_file)
+
+    if args.local_rank == 0 and not evaluate:
+        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache
+
+    # Convert to Tensors and build dataset
+    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
+    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)
+    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
+    if output_mode == "classification":
+        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)
+    else:
+        raise ValueError("No other `output_mode` for Conceptnet.")
+
+    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)
+    return dataset
+
+
+def main():
+    parser = argparse.ArgumentParser()
+
+    # Required parameters
+    parser.add_argument(
+        "--data_dir",
+        default=None,
+        type=str,
+        required=True,
+        help="The input data dir. Should contain the .tsv files (or other data files) for the task.",
+    )
+    parser.add_argument(
+        "--model_name_or_path",
+        default=None,
+        type=str,
+        required=True,
+        help="Path to pretrained model or model identifier from huggingface.co/models",
+    )
+    parser.add_argument(
+        "--output_dir",
+        default=None,
+        type=str,
+        required=True,
+        help="The output directory where the model predictions and checkpoints will be written.",
+    )
+
+    # Other parameters
+    parser.add_argument(
+        "--config_name", default="", type=str, help="Pretrained config name or path if not the same as model_name"
+    )
+    parser.add_argument(
+        "--tokenizer_name",
+        default="",
+        type=str,
+        help="Pretrained tokenizer name or path if not the same as model_name",
+    )
+    parser.add_argument(
+        "--cache_dir",
+        default=None,
+        type=str,
+        help="Where do you want to store the pre-trained models downloaded from s3",
+    )
+    parser.add_argument(
+        "--max_seq_length",
+        default=128,
+        type=int,
+        help="The maximum total input sequence length after tokenization. Sequences longer "
+        "than this will be truncated, sequences shorter will be padded.",
+    )
+    parser.add_argument("--do_train", action="store_true", help="Whether to run training.")
+    parser.add_argument("--do_eval", action="store_true", help="Whether to run eval on the test set.")
+    parser.add_argument(
+        "--evaluate_during_training", action="store_true", help="Rul evaluation during training at each logging step."
+    )
+    parser.add_argument(
+        "--do_lower_case", action="store_true", help="Set this flag if you are using an uncased model."
+    )
+
+    parser.add_argument("--per_gpu_train_batch_size", default=8, type=int, help="Batch size per GPU/CPU for training.")
+    parser.add_argument(
+        "--per_gpu_eval_batch_size", default=8, type=int, help="Batch size per GPU/CPU for evaluation."
+    )
+    parser.add_argument(
+        "--gradient_accumulation_steps",
+        type=int,
+        default=1,
+        help="Number of updates steps to accumulate before performing a backward/update pass.",
+    )
+    parser.add_argument("--learning_rate", default=5e-5, type=float, help="The initial learning rate for Adam.")
+    parser.add_argument("--weight_decay", default=0.0, type=float, help="Weight decay if we apply some.")
+    parser.add_argument("--adam_epsilon", default=1e-8, type=float, help="Epsilon for Adam optimizer.")
+    parser.add_argument("--max_grad_norm", default=1.0, type=float, help="Max gradient norm.")
+    parser.add_argument(
+        "--num_train_epochs", default=3.0, type=float, help="Total number of training epochs to perform."
+    )
+    parser.add_argument(
+        "--max_steps",
+        default=-1,
+        type=int,
+        help="If > 0: set total number of training steps to perform. Override num_train_epochs.",
+    )
+    parser.add_argument("--warmup_steps", default=0, type=int, help="Linear warmup over warmup_steps.")
+
+    parser.add_argument("--logging_steps", type=int, default=500, help="Log every X updates steps.")
+    parser.add_argument("--save_steps", type=int, default=500, help="Save checkpoint every X updates steps.")
+    parser.add_argument(
+        "--eval_all_checkpoints",
+        action="store_true",
+        help="Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number",
+    )
+    parser.add_argument("--no_cuda", action="store_true", help="Avoid using CUDA when available")
+    parser.add_argument(
+        "--overwrite_output_dir", action="store_true", help="Overwrite the content of the output directory"
+    )
+    parser.add_argument(
+        "--overwrite_cache", action="store_true", help="Overwrite the cached training and evaluation sets"
+    )
+    parser.add_argument("--seed", type=int, default=42, help="random seed for initialization")
+
+    parser.add_argument(
+        "--fp16",
+        action="store_true",
+        help="Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit",
+    )
+    parser.add_argument(
+        "--fp16_opt_level",
+        type=str,
+        default="O1",
+        help="For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
+        "See details at https://nvidia.github.io/apex/amp.html",
+    )
+    parser.add_argument("--local_rank", type=int, default=-1, help="For distributed training: local_rank")
+    parser.add_argument("--server_ip", type=str, default="", help="For distant debugging.")
+    parser.add_argument("--server_port", type=str, default="", help="For distant debugging.")
+    args = parser.parse_args()
+
+    if (
+        os.path.exists(args.output_dir)
+        and os.listdir(args.output_dir)
+        and args.do_train
+        and not args.overwrite_output_dir
+    ):
+        raise ValueError(
+            "Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.".format(
+                args.output_dir
+            )
+        )
+
+    # Setup distant debugging if needed
+    if args.server_ip and args.server_port:
+        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
+        import ptvsd
+
+        print("Waiting for debugger attach")
+        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)
+        ptvsd.wait_for_attach()
+
+    # Setup CUDA, GPU & distributed training
+    if args.local_rank == -1 or args.no_cuda:
+        device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
+        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()
+    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
+        torch.cuda.set_device(args.local_rank)
+        device = torch.device("cuda", args.local_rank)
+        torch.distributed.init_process_group(backend="nccl")
+        args.n_gpu = 1
+    args.device = device
+
+    # Setup logging
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,
+    )
+    logger.warning(
+        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
+        args.local_rank,
+        device,
+        args.n_gpu,
+        bool(args.local_rank != -1),
+        args.fp16,
+    )
+
+    # Set seed
+    set_seed(args)
+
+    # Prepare task
+    args.task_name = "conceptnet"
+    args.logging_first_step = True
+
+    args.train_batch_size = args.per_device_train_batch_size = args.per_gpu_train_batch_size
+    args.eval_batch_size = args.per_device_eval_batch_size = args.per_gpu_eval_batch_size=128
+    args.tpu_metrics_debug = None
+    args.save_total_limit = None
+    if args.task_name not in processors:
+        raise ValueError("Task not found: %s" % (args.task_name))
+    processor = processors[args.task_name]()
+    args.output_mode = output_modes[args.task_name]
+    label_list = processor.get_labels()
+    num_labels = len(label_list)
+
+    # Load pretrained model and tokenizer
+    if args.local_rank not in [-1, 0]:
+        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
+
+    config = AutoConfig.from_pretrained(
+        args.config_name if args.config_name else args.model_name_or_path,
+        num_labels=num_labels,
+        finetuning_task=args.task_name,
+        cache_dir=args.cache_dir,
+    )
+    args.model_type = config.model_type
+    tokenizer = AutoTokenizer.from_pretrained(
+        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,
+        do_lower_case=args.do_lower_case,
+        cache_dir=args.cache_dir,
+    )
+    # model = AutoModelForSequenceClassification.from_pretrained(
+    #     args.model_name_or_path,
+    #     from_tf=bool(".ckpt" in args.model_name_or_path),
+    #     config=config,
+    #     cache_dir=args.cache_dir,
+    # )
+
+    # Adapter
+    adapter_args = AdapterArguments(train_adapter=True)
+    model = AutoModelWithHeads.from_pretrained(
+        args.model_name_or_path,
+        from_tf=bool(".ckpt" in args.model_name_or_path),
+        config=config,
+        cache_dir=args.cache_dir,
+    )
+    model.add_classification_head(args.task_name, num_labels=num_labels)
+    # Setup adapters
+    setup_task_adapter_training(model, args.task_name, adapter_args)
+    
+
+    if args.local_rank == 0:
+        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab
+
+    model.to(args.device)
+
+    logger.info("Training/evaluation parameters %s", args)
+    # train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)
+    # eval_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=True)
+    train_dataset = ConceptnetDataset(args, tokenizer=tokenizer, mode="dev") if args.do_eval else None
+    eval_dataset = ConceptnetDataset(args, tokenizer=tokenizer, mode="test") if args.do_eval else None
+
+    # class MyCollator(DataCollator):
+    #     def collate_batch(self, features) -> Dict[str, torch.Tensor]:
+    #         first = features[0]
+    #         print('first', type(first), '\n', first)
+    #         for f in features[:4]:
+    #             print(f[0])
+    #             print(f[1])
+    #             print(f[2])
+    #         key = ['input_ids', 'attention_mask', 'token_type_ids', 'labels']
+    #         batch = {k:[] for k in key}
+    #         # batch = {k: f[i] for f in features for i, k in enumerate(key)}
+
+    #         for f in features:
+    #             batch['input_ids'].append(f[0])
+    #             batch['attention_mask'].append(f[1])
+    #             batch['token_type_ids'].append(f[2])
+    #             batch['labels'].append(f[3])
+    #         batch['input_ids'] = torch.Tensor(batch['input_ids'])
+    #         batch['attention_mask'] = torch.Tensor(batch['attention_mask'])
+    #         batch['token_type_ids'] = torch.Tensor(batch['token_type_ids'])
+    #         batch['labels'] = torch.Tensor(batch['labels'])
+
+    #         return batch
+
+
+    # data_collator = MyCollator()
+    # Initialize our Trainer
+    def compute_metrics(p: EvalPrediction) -> Dict:
+        if args.output_mode == "classification":
+            preds = np.argmax(p.predictions, axis=1)
+        elif args.output_mode == "regression":
+            preds = np.squeeze(p.predictions)
+        return conceptnet_compute_metrics(args.task_name, preds, p.label_ids)
+
+    trainer = Trainer(
+        model=model,
+        args=args,
+        train_dataset=train_dataset,
+        eval_dataset=eval_dataset,
+        compute_metrics=compute_metrics,
+        do_save_full_model=not adapter_args.train_adapter,
+        do_save_adapters=adapter_args.train_adapter,
+    )
+
+    # Training
+    if args.do_train:
+        trainer.train(
+            model_path=args.model_name_or_path if os.path.isdir(args.model_name_or_path) else None
+        )
+        trainer.save_model()
+        # For convenience, we also re-save the tokenizer to the same directory,
+        # so that you can share your model easily on huggingface.co/models =)
+        if trainer.is_world_master():
+            tokenizer.save_pretrained(args.output_dir)
+
+    # # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()
+    # if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):
+    #     # Create output directory if needed
+    #     if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:
+    #         os.makedirs(args.output_dir)
+
+    #     logger.info("Saving model checkpoint to %s", args.output_dir)
+    #     # Save a trained model, configuration and tokenizer using `save_pretrained()`.
+    #     # They can then be reloaded using `from_pretrained()`
+    #     model_to_save = (
+    #         model.module if hasattr(model, "module") else model
+    #     )  # Take care of distributed/parallel training
+    #     model_to_save.save_pretrained(args.output_dir)
+    #     tokenizer.save_pretrained(args.output_dir)
+
+    #     # Good practice: save your training arguments together with the trained model
+    #     torch.save(args, os.path.join(args.output_dir, "training_args.bin"))
+
+    #     # Load a trained model and vocabulary that you have fine-tuned
+    #     model = AutoModelForSequenceClassification.from_pretrained(args.output_dir)
+    #     tokenizer = AutoTokenizer.from_pretrained(args.output_dir)
+    #     model.to(args.device)
+
+    # Evaluation
+    results = {}
+    if args.do_eval and args.local_rank in [-1, 0]:
+        logger.info("*** Evaluate ***")
+
+        # Loop to handle MNLI double evaluation (matched, mis-matched)
+        eval_datasets = [eval_dataset]
+
+        for eval_dataset in eval_datasets:
+            eval_result = trainer.evaluate(eval_dataset=eval_dataset)
+
+            output_eval_file = os.path.join(
+                args.output_dir, f"eval_results_{eval_dataset.args.task_name}.txt"
+            )
+            if trainer.is_world_master():
+                with open(output_eval_file, "w") as writer:
+                    logger.info("***** Eval results {} *****".format(eval_dataset.args.task_name))
+                    for key, value in eval_result.items():
+                        logger.info("  %s = %s", key, value)
+                        writer.write("%s = %s\n" % (key, value))
+
+            results.update(eval_result)
+
+    return results
+
+
+if __name__ == "__main__":
+    main()
diff --git a/run_conceptnet.sh b/run_conceptnet.sh
new file mode 100644
index 00000000..404b4ee3
--- /dev/null
+++ b/run_conceptnet.sh
@@ -0,0 +1,26 @@
+# REQUIRED: DOWNLOAD SST FROME GLUE $ python3 transformers/utils/download_glue_data.py --tasks MNLI
+cd /home/theorist17/projects/adapter/adapter-transformers
+export CONCEPTNET_DIR="/home/theorist17/projects/adapter/neo4j-kbs/classification"
+export TASK_NAME=conceptnet
+
+python3 run_conceptnet.py \
+  --model_name_or_path bert-base-cased \
+  --do_train \
+  --do_eval \
+  --data_dir "$CONCEPTNET_DIR" \
+  --max_seq_length 128 \
+  --per_gpu_train_batch_size 256 \
+  --per_gpu_eval_batch_size 256 \
+  --cache_dir "/home/theorist17/projects/adapter/adapters/$TASK_NAME" \
+  --do_lower_case \
+  --logging_steps 2000 \
+  --save_steps 2000 \
+  --output_dir "/home/theorist17/projects/adapter/adapters/$TASK_NAME" \
+  --overwrite_output_dir \
+  --num_train_epochs 3
+  # --evaluate_during_training \
+  #--overwrite_cache # features!
+  #--train_adapter \
+  #--adapter_config pfeiffer
+
+#  --output_dir /tmp/$TASK_NAME \
\ No newline at end of file
diff --git a/run_cqa_wh.py b/run_cqa_wh.py
index 64d1340a..d9d10dd1 100644
--- a/run_cqa_wh.py
+++ b/run_cqa_wh.py
@@ -34,7 +34,9 @@ from transformers import (
     TrainingArguments,
     set_seed,
     setup_task_adapter_training,
+    AdapterType
 )
+from transformers.modeling_bert import BertForMultipleChoice
 from utils_multiple_choice import MultipleChoiceDataset, Split, processors
 
 
@@ -61,7 +63,7 @@ class ModelArguments:
         default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
     )
     cache_dir: Optional[str] = field(
-        default="/content/gdrive/My Drive/adapter/models/commonsenseqa", metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
+        default="/home/theorist17/projects/adapter/adapters/commonsenseqa", metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
     )
 
 
@@ -145,15 +147,17 @@ def main():
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
         cache_dir=model_args.cache_dir,
     )
-    model = AutoModelForMultipleChoice.from_pretrained(
+    model = BertForMultipleChoice.from_pretrained(
         model_args.model_name_or_path,
         from_tf=bool(".ckpt" in model_args.model_name_or_path),
         config=config,
         cache_dir=model_args.cache_dir,
     )
-
+    
     # Setup adapters
     task_name = data_args.task_name
+    adapter_args.train_adapter = True
+    #model.add_classification_head(data_args.task_name, num_labels=num_labels)
     setup_task_adapter_training(model, task_name, adapter_args)
 
     # Get datasets
@@ -185,7 +189,7 @@ def main():
     def compute_metrics(p: EvalPrediction) -> Dict:
         preds = np.argmax(p.predictions, axis=1)
         return {"acc": simple_accuracy(preds, p.label_ids)}
-
+    
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
diff --git a/run_cqa_wh.sh b/run_cqa_wh.sh
index 8a3c1a97..c6f1786c 100644
--- a/run_cqa_wh.sh
+++ b/run_cqa_wh.sh
@@ -1,36 +1,21 @@
-# # REQUIRED: DOWNLOAD SST FROME GLUE $ python3 transformers/utils/download_glue_data.py --tasks MNLI
-# export DATA_DIR="/content/gdrive/My Drive/adapter/data/commonsenseqa"
-# export TASK_NAME=commonsenseqa
-
-# python3 run_glue_wh.py \
-#   --model_name_or_path bert-base-cased \
-#   --task_name $TASK_NAME \
-#   --do_train \
-#   --do_eval \
-#   --data_dir "$DATA_DIR/$TASK_NAME" \
-#   --max_seq_length 128 \
-#   --per_device_train_batch_size 64 \
-#   --learning_rate 1e-4 \
-#   --num_train_epochs 10.0 \
-#   --output_dir "/content/gdrive/My Drive/adapter/adapters/$TASK_NAME" \
-#   --overwrite_output_dir \
-#   --train_adapter \
-#   --adapter_config pfeiffer
-
-export DATA_DIR="/content/gdrive/My Drive/adapter/data"
+cd "/home/theorist17/projects/adapter/adapter-transformers"
+export DATA_DIR="/home/theorist17/projects/adapter/data"
 export TASK_NAME=commonsenseqa
 
 python3 run_cqa_wh.py \
---task_name $TASK_NAME \
 --model_name_or_path bert-base-cased \
+--task_name $TASK_NAME \
 --do_train \
 --do_eval \
 --data_dir "$DATA_DIR/$TASK_NAME" \
+--max_seq_length 128 \
 --learning_rate 5e-5 \
---num_train_epochs 3 \
---max_seq_length 80 \
---output_dir "/content/gdrive/My Drive/adapter/adapters/$TASK_NAME" \
+--logging_steps 100 \
+--save_steps 100 \
+--num_train_epochs 10 \
+--output_dir "/home/theorist17/projects/adapter/adapters/$TASK_NAME" \
 --per_gpu_eval_batch_size=16 \
 --per_device_train_batch_size=16 \
 --gradient_accumulation_steps 2 \
---overwrite_output
+--overwrite_output \
+--evaluate_during_training
\ No newline at end of file
diff --git a/run_fusion_cqa.py b/run_fusion_cqa.py
index 0cc885d4..0c783ad8 100644
--- a/run_fusion_cqa.py
+++ b/run_fusion_cqa.py
@@ -29,7 +29,8 @@ import numpy as np
 from transformers import (  # setup_task_adapter_training,
     AdapterArguments,
     AutoConfig,
-    AutoModelForSequenceClassification,
+    AdapterConfig,
+    AutoModelForMultipleChoice,
     AutoTokenizer,
     EvalPrediction,
     GlueDataset,
@@ -45,6 +46,7 @@ from transformers import (
     set_seed,
 )
 
+from utils_multiple_choice import MultipleChoiceDataset, Split, processors
 
 logger = logging.getLogger(__name__)
 
@@ -116,8 +118,9 @@ def main():
     set_seed(training_args.seed)
 
     try:
-        num_labels = glue_tasks_num_labels[data_args.task_name]
-        output_mode = glue_output_modes[data_args.task_name]
+        processor = processors[data_args.task_name]()
+        label_list = processor.get_labels()
+        num_labels = len(label_list)
     except KeyError:
         raise ValueError("Task not found: %s" % (data_args.task_name))
 
@@ -137,7 +140,7 @@ def main():
         model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
         cache_dir=model_args.cache_dir,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
+    model = AutoModelForMultipleChoice.from_pretrained(
         model_args.model_name_or_path,
         from_tf=bool(".ckpt" in model_args.model_name_or_path),
         config=config,
@@ -154,63 +157,59 @@ def main():
     base_model.set_adapter_config(AdapterType.text_task, adapter_args.adapter_config)
 
     from transformers.adapter_config import PfeifferConfig
-
-    # from transformers.adapter_config import  HoulsbyConfig
-
-    model.load_adapter(
-        "sentiment/sst-2@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion"
-    )
-    model.load_adapter(
-        "nli/multinli@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion"
-    )
-    model.load_adapter("nli/rte@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion")
-    model.load_adapter("sts/mrpc@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion")
-    model.load_adapter("sts/qqp@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion")
-    model.load_adapter("comsense/cosmosqa@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("comsense/csqa@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("comsense/hellaswag@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("comsense/siqa@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("comsense/winogrande@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("nli/cb@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("nli/sick@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("nli/scitail@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("qa/boolq@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-    model.load_adapter("sentiment/imdb@ukp", "text_task", config=PfeifferConfig(), with_head=False)
-
+    # model.load_adapter(
+    #     "nli/multinli@ukp", "text_task", config=PfeifferConfig(), with_head=False, version="AdapterFusion"
+    # )
+    model.load_adapter("nli/multinli@ukp", "text_task", config=PfeifferConfig(), with_head=False)
+    model.load_adapter("/home/theorist17/projects/adapter/adapters/commonsenseqa/commonsenseqa",
+                       "text_task", config=PfeifferConfig(), with_head=False)
+    model.load_adapter("/home/theorist17/projects/adapter/adapters/conceptnet/conceptnet",
+                       "text_task", config=PfeifferConfig(), with_head=False)
     adapter_names = [
         [
-            "sst_glue",
-            "multinli",
-            "rte",
-            "mrpc",
-            "qqp",
-            "cosmosqa",
-            "csqa",
-            "hellaswag",
-            "socialiqa",
-            "winogrande",
-            "cb",
-            "sick",
-            "scitail",
-            "boolq",
-            "imdb",
+            # "multinli",
+            "commonsenseqa",
+            "conceptnet"
         ]
     ]
 
     model.add_fusion(adapter_names[0], "static", {"regularization": False})
     model.base_model.train_fusion(adapter_names[0])
+    
     # Get datasets
-    train_dataset = GlueDataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
-    eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode="dev") if training_args.do_eval else None
-    test_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode="test") if training_args.do_predict else None
-
+    train_dataset = (
+        MultipleChoiceDataset(
+            data_dir=data_args.data_dir,
+            tokenizer=tokenizer,
+            task=data_args.task_name,
+            max_seq_length=data_args.max_seq_length,
+            overwrite_cache=data_args.overwrite_cache,
+            mode=Split.train,
+        )
+        if training_args.do_train
+        else None
+    )
+    eval_dataset = (
+        MultipleChoiceDataset(
+            data_dir=data_args.data_dir,
+            tokenizer=tokenizer,
+            task=data_args.task_name,
+            max_seq_length=data_args.max_seq_length,
+            overwrite_cache=data_args.overwrite_cache,
+            mode=Split.dev,
+        )
+        if training_args.do_eval
+        else None
+    )
+    
+    def simple_accuracy(preds, labels):
+        return (preds == labels).mean()
+    
     def compute_metrics(p: EvalPrediction) -> Dict:
-        if output_mode == "classification":
-            preds = np.argmax(p.predictions, axis=1)
-        elif output_mode == "regression":
-            preds = np.squeeze(p.predictions)
-        return glue_compute_metrics(data_args.task_name, preds, p.label_ids)
+        preds = np.argmax(p.predictions, axis=1)
+        return {"acc": simple_accuracy(preds, p.label_ids)}
 
+    training_args.eval_during_training = True
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -239,52 +238,43 @@ def main():
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
 
-        # Loop to handle MNLI double evaluation (matched, mis-matched)
-        eval_datasets = [eval_dataset]
-        if data_args.task_name == "mnli":
-            mnli_mm_data_args = dataclasses.replace(data_args, task_name="mnli-mm")
-            eval_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, mode="dev"))
-
-        for eval_dataset in eval_datasets:
-            eval_result = trainer.evaluate(eval_dataset=eval_dataset)
-
-            output_eval_file = os.path.join(
-                training_args.output_dir, f"eval_results_{eval_dataset.args.task_name}.txt"
-            )
-            if trainer.is_world_master():
-                with open(output_eval_file, "w") as writer:
-                    logger.info("***** Eval results {} *****".format(eval_dataset.args.task_name))
-                    for key, value in eval_result.items():
-                        logger.info("  %s = %s", key, value)
-                        writer.write("%s = %s\n" % (key, value))
-
-            eval_results.update(eval_result)
-
-    if training_args.do_predict:
-        logging.info("*** Test ***")
-        test_datasets = [test_dataset]
-        if data_args.task_name == "mnli":
-            mnli_mm_data_args = dataclasses.replace(data_args, task_name="mnli-mm")
-            test_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, mode="test"))
-
-        for test_dataset in test_datasets:
-            predictions = trainer.predict(test_dataset=test_dataset).predictions
-            if output_mode == "classification":
-                predictions = np.argmax(predictions, axis=1)
-
-            output_test_file = os.path.join(
-                training_args.output_dir, f"test_results_{test_dataset.args.task_name}.txt"
-            )
-            if trainer.is_world_master():
-                with open(output_test_file, "w") as writer:
-                    logger.info("***** Test results {} *****".format(test_dataset.args.task_name))
-                    writer.write("index\tprediction\n")
-                    for index, item in enumerate(predictions):
-                        if output_mode == "regression":
-                            writer.write("%d\t%3.3f\n" % (index, item))
-                        else:
-                            item = test_dataset.get_labels()[item]
-                            writer.write("%d\t%s\n" % (index, item))
+        result = trainer.evaluate()
+
+        output_eval_file = os.path.join(training_args.output_dir, "eval_results.txt")
+        if trainer.is_world_master():
+            with open(output_eval_file, "w") as writer:
+                logger.info("***** Eval results *****")
+                for key, value in result.items():
+                    logger.info("  %s = %s", key, value)
+                    writer.write("%s = %s\n" % (key, value))
+
+                eval_results.update(result)
+
+    # if training_args.do_predict:
+    #     logging.info("*** Test ***")
+    #     test_datasets = [test_dataset]
+    #     if data_args.task_name == "mnli":
+    #         mnli_mm_data_args = dataclasses.replace(data_args, task_name="mnli-mm")
+    #         test_datasets.append(GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, mode="test"))
+
+    #     for test_dataset in test_datasets:
+    #         predictions = trainer.predict(test_dataset=test_dataset).predictions
+    #         if output_mode == "classification":
+    #             predictions = np.argmax(predictions, axis=1)
+
+    #         output_test_file = os.path.join(
+    #             training_args.output_dir, f"test_results_{test_dataset.args.task_name}.txt"
+    #         )
+    #         if trainer.is_world_master():
+    #             with open(output_test_file, "w") as writer:
+    #                 logger.info("***** Test results {} *****".format(test_dataset.args.task_name))
+    #                 writer.write("index\tprediction\n")
+    #                 for index, item in enumerate(predictions):
+    #                     if output_mode == "regression":
+    #                         writer.write("%d\t%3.3f\n" % (index, item))
+    #                     else:
+    #                         item = test_dataset.get_labels()[item]
+    #                         writer.write("%d\t%s\n" % (index, item))
     return eval_results
 
 
diff --git a/run_fusion_cqa.sh b/run_fusion_cqa.sh
index 97000676..b9069427 100644
--- a/run_fusion_cqa.sh
+++ b/run_fusion_cqa.sh
@@ -1,16 +1,20 @@
-# REQUIRED: DOWNLOAD SST FROME GLUE $ python3 transformers/utils/download_glue_data.py --tasks MNLI
-export GLUE_DIR="/content/gdrive/My Drive/adapter/data/glue_data"
-export TASK_NAME=SST-2
+cd "/home/theorist17/projects/adapter/adapter-transformers"
+export DATA_DIR="/home/theorist17/projects/adapter/data"
+export TASK_NAME=commonsenseqa
 
 python3 run_fusion_cqa.py \
-  --model_name_or_path bert-base-cased \
-  --task_name $TASK_NAME \
-  --do_train \
-  --do_eval \
-  --data_dir "$GLUE_DIR/$TASK_NAME" \
-  --max_seq_length 128 \
-  --per_device_train_batch_size 32 \
-  --learning_rate 5e-5 \
-  --num_train_epochs 10.0 \
-  --output_dir "/content/gdrive/My Drive/adapter/models/$TASK_NAME" \
-  --overwrite_output_dir
\ No newline at end of file
+--model_name_or_path bert-base-cased \
+--task_name $TASK_NAME \
+--do_train \
+--do_eval \
+--data_dir "$DATA_DIR/$TASK_NAME" \
+--max_seq_length 128 \
+--learning_rate 5e-5 \
+--logging_steps 100 \
+--num_train_epochs 10 \
+--output_dir "/home/theorist17/projects/adapter/fusions/$TASK_NAME/" \
+--per_gpu_eval_batch_size=16 \
+--per_device_train_batch_size=16 \
+--gradient_accumulation_steps 2 \
+--overwrite_output \
+--evaluate_during_training
\ No newline at end of file
diff --git a/run_concept_wh.py b/run_mnli_wh.py
similarity index 100%
rename from run_concept_wh.py
rename to run_mnli_wh.py
diff --git a/run_concept_wh.sh b/run_mnli_wh.sh
similarity index 91%
rename from run_concept_wh.sh
rename to run_mnli_wh.sh
index 728bba50..72527ed1 100644
--- a/run_concept_wh.sh
+++ b/run_mnli_wh.sh
@@ -2,7 +2,7 @@
 export GLUE_DIR="/content/gdrive/My Drive/adapter/data/glue_data"
 export TASK_NAME=MNLI
 
-python3 run_glue_wh.py \
+python3 run_mnli_wh.py \
   --model_name_or_path bert-base-cased \
   --task_name $TASK_NAME \
   --do_train \
diff --git a/src/transformers/__init__.py b/src/transformers/__init__.py
index 2a2ad74d..2a617d2f 100644
--- a/src/transformers/__init__.py
+++ b/src/transformers/__init__.py
@@ -53,6 +53,7 @@ from .data import (
     SquadV1Processor,
     SquadV2Processor,
     glue_convert_examples_to_features,
+    conceptnet_convert_examples_to_features,
     glue_output_modes,
     glue_processors,
     glue_tasks_num_labels,
@@ -61,6 +62,9 @@ from .data import (
     xnli_output_modes,
     xnli_processors,
     xnli_tasks_num_labels,
+    conceptnet_output_modes,
+    conceptnet_processors,
+    conceptnet_tasks_num_labels,
 )
 
 # Files and general utilities
@@ -145,7 +149,7 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 if is_sklearn_available():
-    from .data import glue_compute_metrics, xnli_compute_metrics
+    from .data import glue_compute_metrics, xnli_compute_metrics, conceptnet_compute_metrics
 
 
 # Adapters
@@ -388,6 +392,7 @@ if is_torch_available():
     from .trainer import Trainer, set_seed, torch_distributed_zero_first, EvalPrediction
     from .data.data_collator import DefaultDataCollator, DataCollator, DataCollatorForLanguageModeling
     from .data.datasets import GlueDataset, TextDataset, LineByLineTextDataset, GlueDataTrainingArguments
+    from .data.datasets import ConceptnetDataset
 
     # Benchmarks
     from .benchmark import PyTorchBenchmark, PyTorchBenchmarkArguments
diff --git a/src/transformers/data/__init__.py b/src/transformers/data/__init__.py
index 8d5f6b85..05ab281d 100644
--- a/src/transformers/data/__init__.py
+++ b/src/transformers/data/__init__.py
@@ -13,6 +13,7 @@ from .processors import (
     SquadV1Processor,
     SquadV2Processor,
     glue_convert_examples_to_features,
+    conceptnet_convert_examples_to_features,
     glue_output_modes,
     glue_processors,
     glue_tasks_num_labels,
@@ -20,8 +21,11 @@ from .processors import (
     xnli_output_modes,
     xnli_processors,
     xnli_tasks_num_labels,
+    conceptnet_output_modes,
+    conceptnet_processors,
+    conceptnet_tasks_num_labels,
 )
 
 
 if is_sklearn_available():
-    from .metrics import glue_compute_metrics, xnli_compute_metrics
+    from .metrics import glue_compute_metrics, xnli_compute_metrics, conceptnet_compute_metrics
diff --git a/src/transformers/data/data_collator.py b/src/transformers/data/data_collator.py
index b8f3f571..7ed5879f 100644
--- a/src/transformers/data/data_collator.py
+++ b/src/transformers/data/data_collator.py
@@ -48,7 +48,7 @@ class DefaultDataCollator(DataCollator):
         # So we will look at the first element as a proxy for what attributes exist
         # on the whole batch.
         first = features[0]
-
+        
         # Special handling for labels.
         # Ensure that tensor is created with the correct type
         # (it should be automatically the case, but let's make sure of it.)
diff --git a/src/transformers/data/datasets/__init__.py b/src/transformers/data/datasets/__init__.py
index 74a2147b..b3f8a792 100644
--- a/src/transformers/data/datasets/__init__.py
+++ b/src/transformers/data/datasets/__init__.py
@@ -4,3 +4,4 @@
 
 from .glue import GlueDataset, GlueDataTrainingArguments
 from .language_modeling import LineByLineTextDataset, TextDataset
+from .conceptnet import ConceptnetDataset
diff --git a/src/transformers/data/datasets/conceptnet.py b/src/transformers/data/datasets/conceptnet.py
new file mode 100644
index 00000000..c1736673
--- /dev/null
+++ b/src/transformers/data/datasets/conceptnet.py
@@ -0,0 +1,106 @@
+import logging
+import os
+import time
+from dataclasses import dataclass, field
+from enum import Enum
+from typing import List, Optional, Union
+
+import torch
+from filelock import FileLock
+from torch.utils.data.dataset import Dataset
+
+from ...tokenization_roberta import RobertaTokenizer, RobertaTokenizerFast
+from ...tokenization_utils import PreTrainedTokenizer
+from ...tokenization_xlm_roberta import XLMRobertaTokenizer
+from ..processors.conceptnet import conceptnet_convert_examples_to_features, conceptnet_output_modes, conceptnet_processors
+from ..processors.utils import InputFeatures
+
+from argparse import Namespace
+
+logger = logging.getLogger(__name__)
+
+class Split(Enum):
+    train = "train"
+    dev = "dev"
+    test = "test"
+
+
+class ConceptnetDataset(Dataset):
+    """
+    This will be superseded by a framework-agnostic approach
+    soon.
+    """
+
+    args: Namespace
+    output_mode: str
+    features: List[InputFeatures]
+
+    def __init__(
+        self,
+        args: Namespace,
+        tokenizer: PreTrainedTokenizer,
+        limit_length: Optional[int] = None,
+        mode: Union[str, Split] = Split.train,
+    ):
+        self.args = args
+        self.processor = conceptnet_processors[args.task_name]()
+        self.output_mode = conceptnet_output_modes[args.task_name]
+        if isinstance(mode, str):
+            try:
+                mode = Split[mode]
+            except KeyError:
+                raise KeyError("mode is not a valid split name")
+        # Load data features from cache or dataset file
+        cached_features_file = os.path.join(
+            args.data_dir,
+            "cached_{}_{}_{}_{}".format(
+                mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name,
+            ),
+        )
+        label_list = self.processor.get_labels()
+        self.label_list = label_list
+
+        # Make sure only the first process in distributed training processes the dataset,
+        # and the others will use the cache.
+        lock_path = cached_features_file + ".lock"
+        with FileLock(lock_path):
+
+            if os.path.exists(cached_features_file) and not args.overwrite_cache:
+                start = time.time()
+                self.features = torch.load(cached_features_file)
+                logger.info(
+                    f"Loading features from cached file {cached_features_file} [took %.3f s]", time.time() - start
+                )
+            else:
+                logger.info(f"Creating features from dataset file at {args.data_dir}")
+
+                if mode == Split.dev:
+                    examples = self.processor.get_train_examples(args.data_dir)
+                elif mode == Split.test:
+                    examples = self.processor.get_test_examples(args.data_dir)
+                else:
+                    examples = self.processor.get_train_examples(args.data_dir)
+                if limit_length is not None:
+                    examples = examples[:limit_length]
+                self.features = conceptnet_convert_examples_to_features(
+                    examples,
+                    tokenizer,
+                    max_length=args.max_seq_length,
+                    label_list=label_list,
+                    output_mode=self.output_mode,
+                )
+                start = time.time()
+                torch.save(self.features, cached_features_file)
+                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.
+                logger.info(
+                    "Saving features into cached file %s [took %.3f s]", cached_features_file, time.time() - start
+                )
+
+    def __len__(self):
+        return len(self.features)
+
+    def __getitem__(self, i) -> InputFeatures:
+        return self.features[i]
+
+    def get_labels(self):
+        return self.label_list
diff --git a/src/transformers/data/metrics/__init__.py b/src/transformers/data/metrics/__init__.py
index 6c29c231..00b1eb3c 100644
--- a/src/transformers/data/metrics/__init__.py
+++ b/src/transformers/data/metrics/__init__.py
@@ -83,3 +83,10 @@ if _has_sklearn:
             return {"acc": simple_accuracy(preds, labels)}
         else:
             raise KeyError(task_name)
+
+    def conceptnet_compute_metrics(task_name, preds, labels):
+        assert len(preds) == len(labels)
+        if task_name == "conceptnet":
+            return {"acc": simple_accuracy(preds, labels)}
+        else:
+            raise KeyError(task_name)
\ No newline at end of file
diff --git a/src/transformers/data/processors/__init__.py b/src/transformers/data/processors/__init__.py
index 4cb37faf..db66780c 100644
--- a/src/transformers/data/processors/__init__.py
+++ b/src/transformers/data/processors/__init__.py
@@ -6,3 +6,4 @@ from .glue import glue_convert_examples_to_features, glue_output_modes, glue_pro
 from .squad import SquadExample, SquadFeatures, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features
 from .utils import DataProcessor, InputExample, InputFeatures, SingleSentenceClassificationProcessor
 from .xnli import xnli_output_modes, xnli_processors, xnli_tasks_num_labels
+from .conceptnet import conceptnet_convert_examples_to_features, conceptnet_output_modes, conceptnet_processors, conceptnet_tasks_num_labels
\ No newline at end of file
diff --git a/src/transformers/data/processors/conceptnet.py b/src/transformers/data/processors/conceptnet.py
new file mode 100644
index 00000000..65cd8a28
--- /dev/null
+++ b/src/transformers/data/processors/conceptnet.py
@@ -0,0 +1,166 @@
+# coding=utf-8
+# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
+# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+""" XNLI utils (dataset loading and evaluation) """
+
+
+import logging
+import os
+import pandas as pd
+from .utils import DataProcessor, InputExample, InputFeatures
+from ...file_utils import is_tf_available
+from typing import List, Optional, Union
+if is_tf_available():
+    import tensorflow as tf
+from ...tokenization_utils import PreTrainedTokenizer
+
+logger = logging.getLogger(__name__)
+
+def conceptnet_convert_examples_to_features(
+    examples: Union[List[InputExample], "tf.data.Dataset"],
+    tokenizer: PreTrainedTokenizer,
+    max_length: Optional[int] = None,
+    task=None,
+    label_list=None,
+    output_mode=None,
+):
+    """
+    Loads a data file into a list of ``InputFeatures``
+
+    Args:
+        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.
+        tokenizer: Instance of a tokenizer that will tokenize the examples
+        max_length: Maximum example length. Defaults to the tokenizer's max_len
+        task: GLUE task
+        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method
+        output_mode: String indicating the output mode. Either ``regression`` or ``classification``
+
+    Returns:
+        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``
+        containing the task-specific features. If the input is a list of ``InputExamples``, will return
+        a list of task-specific ``InputFeatures`` which can be fed to the model.
+
+    """
+    return _conceptnet_convert_examples_to_features(
+        examples, tokenizer, max_length=max_length, task=task, label_list=label_list, output_mode=output_mode
+    )
+def _conceptnet_convert_examples_to_features(
+    examples: List[InputExample],
+    tokenizer: PreTrainedTokenizer,
+    max_length: Optional[int] = None,
+    task=None,
+    label_list=None,
+    output_mode=None,
+):
+    if max_length is None:
+        max_length = tokenizer.max_len
+
+    if task is not None:
+        processor = conceptnet_processors[task]()
+        if label_list is None:
+            label_list = processor.get_labels()
+            logger.info("Using label list %s for task %s" % (label_list, task))
+        if output_mode is None:
+            output_mode = conceptnet_output_modes[task]
+            logger.info("Using output mode %s for task %s" % (output_mode, task))
+
+    label_map = {label: i for i, label in enumerate(label_list)}
+
+    def label_from_example(example: InputExample) -> Union[int, float, None]:
+        if example.label is None:
+            return None
+        if output_mode == "classification":
+            return label_map[example.label]
+        elif output_mode == "regression":
+            return float(example.label)
+        raise KeyError(output_mode)
+
+    labels = [label_from_example(example) for example in examples]
+
+    batch_encoding = tokenizer.batch_encode_plus(
+        [(example.text_a, example.text_b) for example in examples], max_length=max_length, pad_to_max_length=True,
+    )
+
+    features = []
+    for i in range(len(examples)):
+        inputs = {k: batch_encoding[k][i] for k in batch_encoding}
+
+        feature = InputFeatures(**inputs, label=labels[i])
+        features.append(feature)
+
+    for i, example in enumerate(examples[:5]):
+        logger.info("*** Example ***")
+        logger.info("guid: %s" % (example.guid))
+        logger.info("features: %s" % features[i])
+
+    return features
+
+class ConceptnetProcessor(DataProcessor):
+    """Processor for the Conceptnet dataset."""
+
+    def __init__(self):
+        pass
+
+    def get_train_examples(self, data_dir):
+        """See base class."""
+        lines = self._read_tsv(os.path.join(data_dir, "train.tsv"))
+        examples = []
+        for (i, line) in enumerate(lines):
+            if i == 0:
+                continue
+            guid = "%s-%s" % ("train", i)
+            text_a = line[1]
+            text_b = line[2]
+            label = line[3]
+            examples.append(InputExample(guid, text_a=text_a, text_b=text_b, label=label))
+        return examples
+
+    def get_test_examples(self, data_dir):
+        """See base class."""
+        lines = self._read_tsv(os.path.join(data_dir, "test.tsv"))
+        examples = []
+        for (i, line) in enumerate(lines):
+            if i == 0:
+                continue
+            guid = "%s-%s" % ("test", i)
+            text_a = line[1]
+            text_b = line[2]
+            label = line[3]
+            examples.append(InputExample(guid, text_a=text_a, text_b=text_b, label=label))
+        return examples
+
+
+    def get_labels(self):
+        """See base class."""
+        return ['Antonym', 'AtLocation', 'CapableOf', 'Causes', 'CausesDesire', 'CreatedBy', 'DefinedAs',
+         'DerivedFrom', 'Desires', 'DistinctFrom', 'Entails', 'EtymologicallyRelatedTo', 'FormOf', 'HasA',
+          'HasContext', 'HasFirstSubevent', 'HasLastSubevent', 'HasPrerequisite', 'HasProperty', 'HasSubevent',
+           'InstanceOf', 'IsA', 'LocatedNear', 'MadeOf', 'MannerOf', 'MotivatedByGoal', 'NotCapableOf', 'NotDesires',
+            'NotHasProperty', 'PartOf', 'ReceivesAction', 'RelatedTo', 'SimilarTo', 'SymbolOf', 'Synonym', 'UsedFor',
+             'dbpedia/capital', 'dbpedia/field', 'dbpedia/genre', 'dbpedia/genus', 'dbpedia/influencedBy', 'dbpedia/knownFor',
+              'dbpedia/language', 'dbpedia/leader', 'dbpedia/occupation', 'dbpedia/product']
+
+
+conceptnet_processors = {
+    "conceptnet": ConceptnetProcessor,
+}
+
+conceptnet_output_modes = {
+    "conceptnet": "classification",
+}
+
+conceptnet_tasks_num_labels = {
+    "conceptnet": 46,
+}
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index f0f1d2b4..17703bc2 100644
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -204,10 +204,10 @@ class Trainer:
         self.compute_metrics = compute_metrics
         self.prediction_loss_only = prediction_loss_only
         self.optimizers = optimizers
-        if tb_writer is not None:
-            self.tb_writer = tb_writer
-        elif is_tensorboard_available() and self.is_world_master():
-            self.tb_writer = SummaryWriter(log_dir=self.args.logging_dir)
+        # if tb_writer is not None:
+        #     self.tb_writer = tb_writer
+        # elif is_tensorboard_available() and self.is_world_master():
+        #     self.tb_writer = SummaryWriter(log_dir=self.args.logging_dir)
         if not is_tensorboard_available():
             logger.warning(
                 "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it."
@@ -488,12 +488,8 @@ class Trainer:
                     len(epoch_iterator) <= self.args.gradient_accumulation_steps
                     and (step + 1) == len(epoch_iterator)
                 ):
-                    # if model.module:
-                    #     model.config = model.module.config
-                    #     model.config.adapter_fusion["regularization"] = model.module.config.adapter_fusion["regularization"]
-                    
                     # apply adapter fusion weight regularization on the value matrix
-                    if hasattr(model.config, "adapter_fusion") and model.config.adapter_fusion["regularization"]:
+                    if hasattr(self.model.config, "adapter_fusion") and self.model.config.adapter_fusion["regularization"]:
                         fusion_reg_loss = get_fusion_regularization_loss(model)
                         fusion_reg_loss.backward()
 
diff --git a/train_multiple_choice.py b/train_multiple_choice.py
index 1318d7ee..aa6eb0ed 100644
--- a/train_multiple_choice.py
+++ b/train_multiple_choice.py
@@ -177,7 +177,7 @@ if task_name not in model.config.adapters.adapter_list(AdapterType.text_task):
     # add a new adapter
     model.add_adapter(
         task_name,
-        AdapterType.text_task
+        AdapterType.text_task,
         config=adapter_args.adapter_config
     )
 # enable adapter training
